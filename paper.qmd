---
title: "Making, Updating, and Querying Causal Models using `CausalQueries`"
abstract: "A guide to the [R]{.proglang} package `CausalQueries`  for making, updating, and querying causal models"
keywords-formatted: [causal models, stan, bayes]
author:
  - name: Till Tietz
    affiliations:
      - name: WZB
        department: IPI
        address: Reichpietschufer 50
        city: Berlin
        country: Germany
        postal-code: 10785
    email: ttietz2014@gmail.com
    orcid: 0000-0002-2916-9059
    url: https://github.com/till-tietz
  - name: Lily Medina
    affiliations:
      - name: UC Berkeley
    email: lily.medina@berkeley.edu
    url: https:/lilymedina.github.io/
  - name: Georgiy Syunyaev
    affiliations:
      - name: Vanderbilt University
      - name: WZB
    orcid: 0000-0002-4391-6313
    email: g.syunyaev@vanderbilt.edu
    url: https://gsyunyaev.com/
  - name: Macartan Humphreys
    affiliations:
      - name: WZB
    orcid: 0000-0001-7029-2326
    email: macartan.humphreys@wzb.eu
    url: https://macartan.github.io/
format:
  jss-pdf:
    keep-tex: true
    include-in-header: supp/in-header.tex
    tbl-cap-location: top
    knitr: 
      opts_chunk:
        prompt: true
        comment: "#>"
        dev: "tikz"
bibliography: supp/bib.bib
fontsize: 11pt
params:
  run: FALSE
execute: 
  eval: true
  warning: false
  error: false
  cache: false
---

```{r}
#| label: preamble
#| include: false

options(kableExtra.latex.load_packages = FALSE)

library(tidyverse)
library(CausalQueries)
library(knitr)
library(rstan)
library(DeclareDesign)
library(kableExtra)
library(tikzDevice)

set.seed(20231018)

```

## Introduction: Causal models {#sec-intro}


`CausalQueries` is an [R]{.proglang} package that lets users make, update, and query causal models. Users  provide a statement that reports a set of binary variables and the relations of causal ancestry between them: which variables are direct causes of other variables, given the other variables in the model. Once provided to `make_model()`, `CausalQueries` generates a  parameter vector that fully describes a probability distribution over all possible types of causal relations between variables ("causal types"), given the causal structure. Given a prior over parameters and data over some or all nodes, `update_model()` deploys a `stan` model in order to generate a posterior distribution over causal models. The function `query_model()` can then be used to ask any causal query of the model, using either the prior distribution, the posterior distribution, or a user-specified candidate vector of parameters.

In the next section we provide a short motivating example. We then describe how the package relates to existing available software. Section @sec-theory gives an overview of the statistical model behind the package. Sections @sec-make, @sec-update, and @sec-query then describe the main functionality for the major operations using the package. We provide further computation details in the final section.


## Motivating example

Before providing details on package functionality we illustrate these three core functions by showing how to use `CuasalQueries` to replicate the analysis in @chickering1996clinician (see also @ii2023). @chickering1996clinician seek to draw inference on causal effects in the presence of imperfect compliance. We have access to an instrument $Z$ (a randomly assigned prescription for cholesterol medication), which is a cause of $X$ (treatment uptake) but otherwise unrelated to $Y$ (cholesterol). We imagine we are interested in three specific queries. The first is the average causal effect of $X$ on $Y$. The second is the average effect for units for which $X=0$ and $Y=0$. The last is the average treatment effect *for* "compliers": units for which $X$ responds positively to $Z$. Thus two of these queries are conditional queries, with one conditional on a counterfactual quantity.

Our data on $Z$, $X$, and $Y$ is complete for all units and looks, in compact form, as follows:

```{r}
#| echo: true

data("lipids_data")

lipids_data

```

With `CausalQueries`, you can create the model, input data to update it, and then query the model for results.

```{r}
#| echo: true
#| eval: false

make_model("Z -> X -> Y; X <-> Y") |>
  update_model(lipids_data, refresh = 0) |>
  query_model(query = "Y[X=1] - Y[X=0]",
              given = c("All",  "X==0 & Y==0", "X[Z=1] > X[Z=0]"),
              using = "posteriors") 
```

```{r}
#| label: tbl-lipids
#| tbl-cap: "Replication of \\citet{chickering1996clinician}."
#| echo: false

if (params$run) {
  
  lipids_model <- 
    make_model("Z -> X -> Y; X <-> Y") |>
    update_model(lipids_data, refresh = 0) 
  
  lipids_model |> 
    readr::write_rds(x = _, file = "saved/lipids_model.rds")
     
  lipids_model |>
    query_model(
      query = "Y[X=1] - Y[X=0]",
      given = c("All",  "X==0 & Y==0", "X[Z=1] > X[Z=0]"),
      using = "posteriors") |>
    readr::write_rds(x = _, file = "saved/lipids_results.rds")

}

lipids_model <- read_rds("saved/lipids_model.rds")

results <- read_rds("saved/lipids_results.rds")

results |>
  dplyr::select(query, given, mean, sd, starts_with("cred")) |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("scale_down")) |> 
  kableExtra::footnote(
    general = "Rows 1 and 2 replicate results in \\\\citet{chickering1996clinician}; row 3 returns inferences for complier average effects.",
    escape = FALSE, fixed_small_size = TRUE, threeparttable = TRUE, general_title = ""
  )

```

The output is a data frame with estimates, posterior standard deviations, and credibility intervals. For example the data frame produced by the code above is shown in @tbl-lipids.

As we describe below the same basic procedure of making, updating, and querying models, can be used (up to computational constraints) for arbitrary causal models, for different types of data structures, and for all causal queries that can be posed of the causal model.

## Connections to existing packages

The literature on causal inference and its software ecosystem are rich and expansive; spanning the social and natural sciences as well as computer science and applied mathematics. In the interest of clarity we thus briefly contextualize `CausalQueries'` scope and functionality within the subset of the causal inference domain addressing the evaluation of causal queries on causal models encoded as directed acyclic graphs (DAGs) or structural equation models (SEMs). We provide a brief tabular overview of relevant software and discuss key connections, advantages and disadvantages with respect to `CausalQueries` in turn. 

+------------------------------+----------------------------------+----------------------+-------------------------+------------------------------------------------------------------------------+
| Software                     | Source                           | Language             | Availability            | Scope                                                                        |
+:=============================+:=================================+:=====================+:========================+:============================================================================:+
| **autobounds**               | @Duarte2023autobounds            | **Python**           | not readily installable | - bounding causal effects                                                    |
|                              |                                  |                      |                         | - partial identification                                                     |
|                              |                                  |                      |                         | - DAG canonicalization                                                       |
|                              |                                  |                      |                         | - binary data                                                                |
+------------------------------+----------------------------------+----------------------+-------------------------+------------------------------------------------------------------------------+
| **causalnex**                | @Beaumont2021causalnex           | **Python**           | pip installable         | - causal structure learning                                                  | 
|                              |                                  |                      |                         | - querying marginal distributions                                            |
|                              |                                  |                      |                         | - discrete data                                                              |
+------------------------------+----------------------------------+----------------------+-------------------------+------------------------------------------------------------------------------+
| **causaloptim**              | @sachs2023causaloptim            | **R**                | CRAN                    | - bounding causal effects                                                    |
|                              |                                  |                      |                         | - non-identified queries                                                     |
|                              |                                  |                      |                         | - binary data                                                                |
+------------------------------+----------------------------------+----------------------+-------------------------+------------------------------------------------------------------------------+
| **pclag**                    | @kalisch2012pclag                | **R**                | CRAN                    | - causal structure learning                                                  |
|                              |                                  |                      |                         | - ATEs under linear conditional expectations and no hidden selection         |
+------------------------------+----------------------------------+----------------------+-------------------------+------------------------------------------------------------------------------+

One of the most comprehensive pieces of software in the causal modeling domain is `causalnex`. It provides a feature rich and highly optimized set of tools to learn, update and query causal models using discrete data. While avoiding the rich model parameterization via principal strata (nodal types) employed by `CausalQueries`, allows `causalnex` to easily handle non-binary data and scale to large causal models; it substantially restricts the set of possible queries that can be evaluated and prior knowledge that can be specified on models. `causalnex` is in this sense akin to machine learning approaches to causal inference, focusing on causal structure learning in variable rich but potentially domain knowledge poor settings and the evaluation of simple queries over marginal distributions on learned DAGs. The rich model structure employed by `CausalQueries` by contrast makes it highly suited to problems where answers to complex causal queries are sought in relatively more domain knowledge abundant settings. Like `causalnex` `pclag` places particular emphasis on causal structure learning, utilizing the resultant DAGs to recover ATEs across all learned markov-equivalent classes implied by observed data that satisfy linearity of conditional expectations. This approach again is far more restrictive than `CausalQueries` in the the DAGs and queries it allows. The methods bearing the highest resemblance to `CausalQueries` with respect to model definition are `autobounds` and `causaloptim`. Dealing with binary causal models, their definitions of principal strata (nodal types) and the resultant set of causal relations on the DAG (causal types) are identical to those of `CausalQueries`. Differences in model definition arise only with respect to disturbance terms and confounding being defined implicitly via main nodes and edges in `CausalQueries` vs explicitly via separate disturbance nodes in `autobounds` and `causaloptim`. A key consequence of this is that while `CausalQueries` assumes canonical form for input DAGs, `autobounds` and `causaloptim` facilitate cannonicalization. The essential difference between the methods; however, lies in their approach to evaluating queries. While `autobounds` and `causaloptim` conceptualize query evaluation as constrained polynomial and linear optimization problems respectively, `CausalQueries` utilizes Bayesian inference to generate a posterior over the causal model (@zhang2022pci propose a Bayesian model similar to `CausalQueries` but provide no software implementation). The polynomial and linear programming approach to querying is in principle suited to handling somewhat larger causal models, though given their similarity in model parameterization `autobounds`, `causaloptim` and `CausalQueries` face similar constraints induced by parameter spaces expanding rapidly with model size. The Bayesian approach to model updating and querying holds the major efficiency advantage that a model can be updated once and queried infinitely, while expensive optimization runs are required for each separate query in `autobounds` and `causaloptim`. This in addition to allowing for full Bayesian inference and the ablity to generate complete posteriors over causal queries instead of bounds is a major advantage of `CausalQueries`. It is further unclear whether polynomial and linear optimization can effectively handle complex causal queries with nested do operations. `CausalQueries` fundamentally combines the feature richness of more general causal inference software with the modeling and querying flexibility afforded by rich model parameterizations and fully Bayesian inference.  


```{r}
#| echo: true
#| results: markup
#| eval: false
#| include: false

make_model("A -> B -> C -> D -> E")$parameters_df |> nrow()

make_model("A -> E <- B; C->E<-D")$parameters_df |> nrow()
```

The particular strength of `CausalQueries` is to allow users to specify arbitrary DAGs, arbitrary queries over nodes in those DAGs, and use the same canonical procedure to learn about those queries whether or not the queries are identified. Thus in principle if researchers are interest in learning about a quantity like the local average treatment effect and their model in fact satisfies the conditions in @angrist1996identification, then updating will recover valid estimates even if researchers are unaware that the local average treatment effect is identified and are ignorant of the estimation procedure proposed by @angrist1996identification.

There are two broad limitation on the sets of models handled natively by `CausalQueries`. First `CausalQueries` is designed for models with a relatively small number over binary nodes. Because there is no compromise made on the space of possible causal relations implied by a given model, the parameter space grows very rapidly with the complexity of the causal model. The complexity also depends on the causal structure and grows rapidly with the number of parents affecting a given child. A chain model of the form $A \rightarrow B \rightarrow C \rightarrow D \rightarrow E$ has just 40 parameters. A model in which $A, B, C, D$ are all direct of $E$ has 65544 parameters. Moving from binary to non binary nodes has similar effects. The restriction to binary nodes is for computational and not conceptual reasons. In fact it is possible to employ `CausalQueries` to answer queries from models with nonbinary nodes but in general the computational costs make analysis of these model prohibitive.^[For more on computation constraints and strategies to update and query large models see the associated package `CausalQueriesTools`. The core approach used here is to divde large causal models into modules, update on modules and reassemble to pose queries.] <!-- # should we mention computational constraints more explicitly here? >> i.e. constraints on memory as calculated in the CausalQueriesTools documentation. Could also be a place to mention the package + frankensteining methods though that might better be placed in the updating models section -->

Second, the package is geared for learning about populations from samples of units that are independent of each other and are independently randomly sampled from populations. Thus the basic set up does not address problems of sampling, clustering, hierarchical structures, or purposive sampling, for example. The broader framework can however be used for these purposes (see section 9.4 of @ii2023). The target of inference is usually case level quantities or population quantities and not sample quantities.  

## Statistical model {#sec-theory}

The core conceptual framework is described in Pearl's *Causality* [@pearl2009causality] but can be summarized as follows (using the notation used in @ii2023):

```{=tex}
\begin{definition}
  
  A ``\textbf{causal model}'' is:
  \begin{enumerate}
    \item an ordered collection of "endogenous nodes" $Y = \{Y_1, Y_2, \dots, Y_n\}$
    \item an ordered collection of "exogenous nodes" $\Theta = \{\theta^{Y_1}, \theta^{Y_1}, \dots, \theta^{Y_n}\}$
    \item a collection of functions $F = \{f_{Y_1}, f_{Y_2}, \dots, f_{Y_n}\}$ specifying, for each node $j$, how outcome $y_j$ depends on $\theta_j$ and realizations of endogenous nodes prior to $j$.
    \item a probability distribution over $\Theta$, $\lambda$
  \end{enumerate}
  
\end{definition}
```
In the usual case we take the endogenous nodes to be binary.[^1] When we specify a causal structure we specify which endogenous nodes are (possibly) direct causes of a node, $Y_j$, given other nodes in the model. These nodes are called the parents of $Y_j$, $PA_j$ (we use upper case $PA_j$ to indicate the collection of nodes and lower case $pa_j$ to indicate a particular set of values that these nodes might take on). With discrete valued nodes, it is possible to identify all possible ways that a node might respond to its parents.  We refer to the ways that a node responds and the nodes  "nodal type." The set of nodal types  corresponds to principal strata familiar, for instance, in the study of instrumental variables [@frangakis2002principal].    

If node $Y_i$ can take on $k_i$ possible values then the set of possible values that can be taken on by parents of $j$ is $m :=\prod_{i\in PA_j}k_i$. Then there are $k_j^{m}$ different ways that a node might respond to its parents.  In the case of binary nodes this becomes $2^{\left(2^{|PA_j|}\right)}$. Thus for an endogenous node with no parents there are 2 nodal types, for a binary node with one binary parent there are four types, for a binary node with 2 parents there are 16, and so on.

The set of all possible causal reactions of a given unit to all possible values of parents is then given by its collection of nodal types at each node. We call this collection a unit's "causal type", $\theta$.

The approach used by `CausalQueries` is to let the domain of $\theta^{Y_j}$ be coextensive with the number of nodal types for $Y_j$. Function $f^j$ then determines the value of $y$ by simply reporting the value of $Y_j$ implied by the nodal type and the values of the parents of $Y_j$. Thus if $\theta^j_{pa_j}$ is the value for $j$ when parents have values $pa_j$, then we have simply that $f_{y_j}(\theta^{j}, pa_j) = \theta^j_{pa_j}$. The practical implication is that, given the causal structure, learning about the model reduces to learning about the distribution, $\lambda$, over the nodal types.

In cases in which there is no unobserved confounding, we take the probability distributions over the nodal types for different nodes to be independent: $\theta^i \perp\!\!\! \perp \theta^j, i\neq j$. In this case we use a categorical distribution to specify the ${\lambda^j}' := \Pr(\theta^j = {\theta^j}')$. From independence then we have that the probability of a given causal type $\theta'$ is simply $\prod_{i=1}^n {\lambda^i}'$.

In cases in which there is confounding, the  logic is essentially the same except that we need to specify enough parameters to capture the joint distribution over nodal types for different nodes.

We make use of the causal structure to simplify. As an example, for the lipids model, the  full joint distribution of nodal types can be simplified as in @eq-join.


$$
\Pr(\theta^Z = \theta^Z_1, \theta^X = \theta^X_{10}, \theta^Y = \theta^Y_{11}) = 
\Pr(\theta^Z = \theta^Z_1)\Pr(\theta^X = \theta^X_{10})\Pr(\theta^Y = \theta^Y_{11}|\theta^X = \theta^X_{10})
$$ {#eq-join}

And so for this model $\lambda$ would include  parameters that represent $\Pr(\theta^Z)$ and $\Pr(\theta^X)$ but also the conditional probability $\Pr(\theta^Y|\theta^X)$:

$$
\Pr(\theta^Z = \theta^Z_1, \theta^X = \theta^X_{10}, \theta^Y = \theta^Y_{11}) = 
\lambda^Z_1\lambda^X_{10}\lambda^{Y|\theta^X_{10}}_{11}
$$ {#eq-join2}

Representing beliefs *over causal models* thus requires specifying a probability distribution over $\lambda$. This might be a degenerate distribution if users want to specify a particular model. `CausalQueries` allows users to specify parameters, $\alpha$ of a Dirichlet distribution over $\lambda$. If all entries of $\alpha$ are 0.5 this corresponds to Jeffrey's priors. The default behavior is for `CausalQueries` to assume a uniform distribution -- that is, that all nodal types are equally likely -- which corresponds to $\alpha$ being a vector of 1s.


Updating is then done with respect to beliefs over $\lambda$. In the Bayesian approach we have simply:

$$p(\lambda'|D) = \frac{p(D|\lambda')p(\lambda')}{\int_{\lambda^{''}} p(D|\lambda'')p(\lambda'')}$$ 

$p(D|\lambda')$ is calculated under the assumption that units are exchangeable and independently drawn, and of course. In practice this means that the probability that two units have causal types $\theta_i$ and $\theta_j$ is simply $\lambda'_i\lambda'_j$. Since a causal type fully determines an outcome vector $d = \{y_1, y_2,\dots,y_n\}$, the probability of a given outcome ("event"), $w_d$, is given simply by the probability that the causal type is among those that yield outcome $d$. Thus from $\lambda'$ we can calculate a vector of event probabilities, $w(\lambda)$, for each vector of outcomes, and under independence we have:

$$D \sim \text{Mulitinomial}(w(\lambda), N)$$

Thus for instance in the case of a $X \rightarrow Y$ model, and letting $w_{xy}$ denote the probability of a data type $X=x, Y=y$, the event probabilities are:

$$w(\lambda) = \left\{\begin{array}{ccc} w_{00} & = & \lambda^X_0(\lambda^Y_{00} + \lambda^Y_{01})\\ 
w_{01} & = & \lambda^X_0(\lambda^Y_{11} + \lambda^Y_{10})\\
w_{10} & = & \lambda^X_1(\lambda^Y_{00} + \lambda^Y_{10})\\
w_{11} & = & \lambda^X_1(\lambda^Y_{11} + \lambda^Y_{01})\end{array} \right.$$


For concreteness: table @lipidspar illustrates key  values for the lipids model. We see here that we have two types for node $Z$,  four for $X$ (representing the strata familiar from instrumental variables analysis: never takers, always takers, defiers, and compliers)  and 4 for $Y$. For $Z$ and $X$ we have parameters corresponding to probability of these nodal types. For instance `Z.0` is the probability that $Z=1$. `Z.1` is the complementary probability that $Z=1$.  Things are little more complicated for distributions on nodal types for $Y$ however: because of confounding between $X$ and $Y$ we have parameters that capture the conditional probability of the nodal types for $Y$ *given* the nodal types for $X$.  We see there are four sets of these parameters. 




```{r lipidspar, echo = FALSE}
with_pars <- 
  lipids_model |>
  set_parameters(param_type = "prior_draw") 

with_pars$parameters_df |>
  
  select(node,  nodal_type, param_set, param_names, param_value) |> 
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "",
    caption = "Nodal types and parameters for Lipids model") 

```

The final column shows a sample set of parameter values.  Together  the parameters describe a full joint probability distribution over types for $Z$, $X$ and $Y$ that is faithful to the graph. 

From these we can calculate the probability of each data type. For instance the probability of data type $Z=0, X=0, Y=0$ is: 

$$w_{000}=\Pr(Z=0, X=0, Y=0) = \lambda^Z_0\left(\lambda^X_{00}(\lambda^{Y|\lambda^X_{00}}_{00}+\lambda^{Y|\lambda^X_{00}}_{01}) + \lambda^X_{01}(\lambda^{Y|\lambda^X_{01}}_{00}+\lambda^{Y|\lambda^X_{01}}_{01})\right)$$
In practice `CausalQueries` uses a matrix `parmap` that maps from parameters into data types.  


The value of the `CausalQueries` package is to allow users to specify *arbitrary* models of this form, figure out all the implied nodal types and causal types, and then update given priors and data by calculating event probabilities implied by all possible parameter vectors and in turn the likelihood of the data given the model. In addition, the package allows for arbitrary querying of a model to assess the values of estimands of interest that a re function of the values or counterfactual values of nodes conditional on values or counterfactual values of nodes.



In the next sections we review key functionality for making, updating and querying causal models.

## Making models {#sec-make}

A model is defined in one step in `CausalQUeries` using a `dagitty` syntax in which the structure of the model is provided as a statement.

For instance:

```{r}
#| echo: true
#| results: markup

model <- make_model("X -> M -> Y <- X")
```

The statement in quotes, `"X -> M -> Y <- X"`, provides the names of nodes. An arrow ("`->`" or "`<-`") connecting nodes indicates that one node is a potential cause of another, i.e. whether a given node is a "parent" or "child" of another.

Formally a statement like this is interpreted as:

1.  Functional equations:

    -   $Y = f(M, X, \theta^Y)$
    -   $M = f(X, \theta^M)$
    -   $X = \theta^X$

2.  Distributions on $\Theta$:

    -   $\Pr(\theta^i = \theta^i_k) = \lambda^i_k$

3.  Independence assumptions:\

    -   $\theta_i \perp\!\!\! \perp \theta_j, i\neq j$

where function $f$ maps from the set of possible values of the parents of $i$ to values of node $i$ given $\theta^i$ as described above.

In addition, as we did in the @chickering1996clinician example, it is  possible to use two headed arrows (`<->`) to indicate "unobserved confounding", that is, the presence of an unobserved variable that might influence observed variables. In this case condition 3 above is relaxed and the exogeneous nodes associated with confounded variables have a joint distribution. We describe how this is done in greater detail in section @sec-confounding.

### Graphing

Plotting the model can be useful to check that you have defined the structure of the model correctly. `CausalQueries` provides simple graphing tools that draw on functionality from the `dagitty`, `ggplot2`, and `ggdag` packages.

Once defined, a model can be graphed by calling the `plot()` method defined for the objects with class `causal_model` produced by `make_model()` function.

```{r}
#| echo: true
#| eval: false

make_model("X -> M -> Y <- X; Z -> Y") |>
  plot()

```

Alternatively you can provide a number of options to the `plot()` call that will be passed to `CausalQueries:::plot_dag` via the method.

```{r}
#| echo: true
#| eval: false

make_model("X -> M -> Y <- X; Z -> Y") |>
  plot(x_coord = 1:4,
       y_coord = c(1.5,2,1,2),
       textcol = "white",
       textsize = 3,
       shape = 18,
       nodecol = "grey",
       nodesize = 12)

```

The graphs produced by the two calls above are shown in @fig-plots. In both cases the resulting plot will have class `c("gg", "ggplot")` and so will accept any additional modifications available via `ggplot2` package.

```{r}
#| label: fig-plots
#| echo: false
#| fig-cap: "Examples of model graphs."
#| fig-subcap:
#|   - "Without options"
#|   - "With options"
#| fig-pos: 'h'
#| layout-ncol: 2

make_model("X -> M -> Y <- X; Z -> Y") |>
  plot()

make_model("X -> M -> Y <- X; Z -> Y") |>
  plot(x_coord = 1:4,
       y_coord = c(1.5,2,1,2),
       textcol = "white",
       textsize = 3,
       shape = 18,
       nodecol = "grey",
       nodesize = 12)

```

### Model characterization

When a model is defined, a set of objects is generated. These are the key quantities that are used for all inference. The table below summarizes the core components of a model, providing a brief explanation for each one.

The first element is a `statement` which defines how the nodes in the model are related, specified by the user using `dagitty` syntax. The second element, `dag`, is a dataframe that outlines the implied parent-child relationships within the model. `nodes` is simply a list of the names of the implied nodes in the model. Lastly, `parents_df,` is a table listing the nodes, indicating if they are "root" nodes (notes with no parents among the set of specified nodes), and showing how many parents each node has.

The model includes additional elements, `nodal_types,` `parameters_df,` and `causal_types,` which we explain later in detail.

+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Element               | Description                                                                                                                                                         |
+=======================+=====================================================================================================================================================================+
| `statement`           | A character string that describes directed causal relations between variables in a causal model, where arrows denote that one node is a potential cause of another. |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `dag`                 | A data frame with columns 'parent' and 'children' indicating how nodes relate to each other.                                                                        |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `nodes`               | A list containing the nodes in the model.                                                                                                                           |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `parents_df`          | A table listing nodes, whether they are root nodes or not, and the number of parents they have.                                                                     |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `nodal_types`         | A list with the nodal types in the model. See @sec-nodal-types for more details.                                                                                    |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `parameters_df`       | A data frame linking the model's parameters with the nodal types of the model, as well as the family to which they belong. See @sec-param-df for more details.      |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `causal_types`        | A data frame listing causal types and the nodal types that produce them. (See Causal Types Section)                                                                 |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Core Elements of a Causal Model. {tbl-colwidths="\[30,70\]"}

After updating a model, two additional components are attached to it:

-   A posterior distribution of the parameters in the model, generated by stan. This distribution reflects the updated parameter values.

-   A list of objects, which we refer to as `stan_objects`. The `stan_objects` will, at a minimum, include the distribution of nodal types and the data used for updating the model

Optionally, users can choose whether to keep and append additional elements to the `stan_objects.` For instance, they can specify whether to include `w`, which maps parameters to event probabilities, as well as the `stanfit` object, the output generated by stan

The table below summarizes the objects attached to the model after updating.

+--------------------------+------------------------------------------------------------------------------------------+
| Element                  | Description                                                                              |
+==========================+==========================================================================================+
| `posterior_distribution` | The posterir distribution of the updated parameters generated by stan.                   |
+--------------------------+------------------------------------------------------------------------------------------+
| `stan_objects`           | A list of objects, including, at a minimum, `type_distribution` and `data`.              |
+--------------------------+------------------------------------------------------------------------------------------+
| `data`                   | The data used for updating the model, appended to `stan_objects.`                        |
+--------------------------+------------------------------------------------------------------------------------------+
| `type_distribution`      | The updated distribution of the nodal types, appended to `stan_objects.`                 |
+--------------------------+------------------------------------------------------------------------------------------+
| `w`                      | A mapping from parameters to event probabilities, optionally appended to `stan_objects.` |
+--------------------------+------------------------------------------------------------------------------------------+
| `stan_fit`               | The `stanfit` object generated by stan. This is optionally appended to `stan_objects.`   |
+--------------------------+------------------------------------------------------------------------------------------+

: Additional Elements. {tbl-colwidths="\[30,70\]"}

#### Parameters data frame {#sec-param-df}

When a model is created, `CausalQueries` attaches a "parameters data frame" which keeps track of model parameters, which belong together in a family, and how they relate to causal types. This becomes especially important for more complex models with confounding that might involve more complicated mappings between parameters and nodal types. In the case with no confounding the nodal types *are* the parameters; in cases with confounding there are generally  more parameters than nodal types.

For instance:

```{r}
#| label: params-df
#| echo: true

make_model("X -> Y")$parameters_df
```

```{r}
#| label: tbl-params-df
#| echo: false
#| tbl-cap: "Example of Parameters Data Frame"

make_model("X -> Y")$parameters_df |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("scale_down"))
```

Produces parameters data frame shown in @tbl-params-df. Each row in the data frame corresponds to a single parameter.

The columns of the parameters data frame are understood as follows:

-   `param_names` gives the name of the parameter, in shorthand. For instance the parameter $\lambda^X_0 = \Pr(\theta^X = \theta^X_0)$ has `par_name` `X.0`. See section \@ref(notation) for a summary of notation.
-   `param_value` gives the (possibly default) parameter values. These are probabilities.\
-   `param_set` indicates which parameters group together to form a simplex. The parameters in a set have parameter values that sum to 1. In this example $\lambda^X_0 + \lambda^X_1 = 1$.
-   `node` indicates the node associated with the parameter. For parameter `\lambda^X_0` this is $X$.
-   `nodal_type` indicates the nodal types associated with the parameter.
-   `gen` indicates the place in the partial causal ordering (generation) of the node associated with the parameter
-   `priors` gives (possibly default) Dirichlet priors arguments for parameters in a set. Values of 1 (.5) for all parameters in a set implies uniform (Jeffrey's) priors over this set.

Below we will see examples where the parameter data frame helps keep track of parameters that are created when confounding is added to a model.

#### Nodal types {#sec-nodal-types}

As described above, two units have the same *nodal type* at node $Y$, $\theta^Y$, if their outcome at $Y$ responds in the same ways to parents of $Y$.

A binary node with $k$ binary parents has $2^{2^k}$ nodal types. The reason is that with $k$ parents, there are $2^k$ possible values of the parents and so $2^{2^k}$ ways to respond to these possible parental values. As a convention we say that a node with no parents has two nodal types (0 or 1).

When a model is created the full set of nodal types is identified. These are stored in the model. The labels for these nodal types indicate how the unit responds to values of parents. 

For instance, consider the model with two parents $X \rightarrow Y \leftarrow M.$ In such a case, the nodal types of $Y$ will have subscripts with four digits, with each digit representing one of the possible combinations of values that $Y$ can take, given the values of its parents $X$ and $M.$ These combinations include the value of $Y$ when:

-   X = 0 and M = 0,
-   X = 0 and M = 1,
-   X = 1 and M = 0,
-   X = 1 and M = 1.

As the number of parents increases, keeping track of what each digit represents becomes more difficult. For instance, if $Y$ had three parents, its nodal types would have subscripts of eight digits, each associated with the value that $Y$ would take for each combination of the three parents. The `interpret_type` function provides a clear map to identify what each digit in the subscript represents. See the examples below for models with two and three parents.

Interpretations are autimatically provided as part of the model object. A user can  see them like this.

```{r}
#| label: nodal-types
#| echo: true
#| eval: false

make_model("X -> Y")$nodal_types
```

```{r}
#| label: nodal-types-nointerpret
#| echo: false
#| eval: true

# make_model("X -> Y")$nodal_types |> `attr<-`("interpret", NULL)
make_model("X -> Y")$nodal_types 
```

The `interpret_type` function can also be called by the user  to obtain interpretations for the nodal types of each node in the model. 

```{r}
#| label: lookup-types1
#| echo: true

interpretations <- 
  make_model("X -> Y <- M ") |> 
  interpret_type()

interpretations$Y

```

```{r}
#| label: lookup-types2
#| echo: true

interpretations <- 
  make_model("X -> Y <- M; W -> Y") |> 
  interpret_type()

interpretations$Y
```


#### Causal types

Causal types are collections of nodal types. Two units are of the same *causal type* if they have the same nodal type at every node. For example in a $X \rightarrow M \rightarrow Y$ model, $\theta = (\theta^X_0, \theta^M_{01}, \theta^Y_{10})$ is a type that has $X=0$, $M$ responds positively to $X$, and $Y$ responds positively to $M$.

When a model is created, the full set of causal types is identified. These are stored in the model object:

```{r}
#| label: causal-types
#| echo: true

lipids_model$causal_types |> head()

```

In the lipids model there are $2\times 4\times 4 = 32$ causal types. A model with $n_j$ nodal types at node $j$ has $\prod_jn_j$ causal types. Thus the set of causal types can be large. 

Knowledge of a causal type tells you what values a unit would take, on all nodes, whether or not there are  interventions. For example for a model $X \rightarrow M \rightarrow Y$ a type $\theta = (\theta^X_0, \theta^M_{01}, \theta^Y_{10})$ would imply data $(X=0, M=0, Y=1)$ absent any intervention. (The converse of this, of course, is the key to updating: observation of data $(X=0, M=0, Y=1)$ result in more weight placed on $\theta^X_0$, $\theta^M_{01}$, and $\theta^Y_{10})$.) The general approach used by `CausalQueries` for calculating outcomes from causal types is given insection #propogation.


#### Parameter matrix

The parameters data frame keeps track of parameter values and priors for parameters but it does not provide a mapping between parameters and the probability of causal types.

The parameter matrix ($P$ matrix) can be added to the model to provide this mapping. The $P$ matrix has a row for each parameter and a column for each causal type. For instance:

```{r}
#| label: get-param-matrix
#| echo: true

make_model("X -> Y") |> get_parameter_matrix()

```

The probability of a causal type is given by the product of the parameters values for parameters whose row in the $P$ matrix contains a 1.

Below we will see examples where the $P$ matrix helps keep track of parameters that are created when confounding is added to a model. CROSS REFERENCE

To speed up different operations it is sometimes useful to have P added to the model:

-   `set_parameter_matrix`...

### Tailoring models

#### Setting restrictions {#restrictions}

When a model is defined, the complete set of possible causal relations are worked out. This set can be very large.

Sometimes for theoretical or practical reasons it is useful to constrain the set of types. In `CausalQueries` this is done at the level of nodal types, with restrictions on causal types following from restrictions on nodal types.

For instance to impose an assumption that $Y$ is not decreasing in $X$ we generate a restricted model as follows:

```{r}
#| echo: true
#| eval: false

model <- make_model("X -> Y") |> set_restrictions("Y[X=1] < Y[X=0]")
```

or

```{r}
#| echo: true
#| eval: true

model <- make_model("X -> Y") |> set_restrictions(decreasing("X", "Y"))
```

Viewing the resulting parameter matrix we see that both the set of parameters and the set of causal types are now restricted:

```{r}
#| label: get-param-matrix-restrict
#| echo: true

get_parameter_matrix(model)

```

Here and in general, setting restrictions typically involves using causal syntax; see Section \@ref(sec-syntax) for a guide the syntax used by `CausalQueries`.

<!-- GS: Do we need to add this section? -->

Note:

-   Restrictions have to operate on nodal types: restrictions on *levels* of endogenous nodes aren't allowed. This, for example, will fail: `make_model("X -> Y") |> set_restrictions(statement =  "(Y == 1)")`. The reason is that it requests a correlated restriction on nodal types for `X` and `Y` which involves undeclared confounding.
-   Restrictions implicitly assume fixed values for *all* parents of a node. For instance: `make_model("A -> B <- C") |> set_restrictions("(B[C=1]==1)")` is interpreted as shorthand for the restriction `"B[C = 1, A = 0]==1 | B[C = 1, A = 1]==1"`.
-   To place restrictions on multiple nodes at the same time, provide these as a vector of restrictions. This is not permitted: `set_restrictions("Y[X=1]==1 & X==1")`, since it requests correlated restrictions. This however is allowed: `set_restrictions(c("Y[X=1]==1", "X==1"))`.\
-   Use the `keep` argument to indicate whether nodal types should be dropped (default) or retained.
-   Restrictions can be set using nodal type labels.

```{r}
#| label: set-restrictions1
#| echo: true
#| eval: false

make_model("S -> C -> Y <- R <- X; X -> C -> R") |>
  set_restrictions(labels = list(C = "1000", R = "0001", Y = "0001"), 
                   keep = TRUE)
```

-   Wild cards can be used in nodal type labels:

```{r}
#| label: set-restrictions2
#| echo: true
#| eval: false

make_model("X -> Y") |>
  set_restrictions(labels = list(Y = "?0"))
```

<!-- # you can also specify a given when setting restrictions e.g.  -->

<!-- model <- make_model("X -> Y -> Z; X <-> Z") %>% -->

<!--   set_restrictions(list(decreasing('X','Y'), decreasing('Y','Z')), given = c(NA,'X.0')) -->

-   adding "given" for restrictions with models with confounding

#### Allowing confounding {#confounding}

(Unobserved) confounding between two nodes arises when the nodal types for the nodes are not independently distributed.

In the $X \rightarrow Y$ graph, for instance, there are 2 nodal types for $X$ and 4 for $Y$. There are thus 8 joint nodal types (or causal types):

+------------------------------+----------------------------------+----------------------------------+----------------------+
| $\theta^Y$ **\\** $\theta^X$ | **0**                            | **1**                            | $\sum$               |
+:============================:+:================================:+:================================:+:====================:+
| **00**                       | $\Pr(\theta^X_0, \theta^Y_{00})$ | $\Pr(\theta^X_1, \theta^Y_{00})$ | $\Pr(\theta^Y_{00})$ |
+------------------------------+----------------------------------+----------------------------------+----------------------+
| **10**                       | $\Pr(\theta^X_0, \theta^Y_{10})$ | $\Pr(\theta^X_1, \theta^Y_{10})$ | $\Pr(\theta^Y_{10})$ |
+------------------------------+----------------------------------+----------------------------------+----------------------+
| **01**                       | $\Pr(\theta^X_0, \theta^Y_{01})$ | $\Pr(\theta^X_1, \theta^Y_{01})$ | $\Pr(\theta^Y_{01})$ |
+------------------------------+----------------------------------+----------------------------------+----------------------+
| **11**                       | $\Pr(\theta^X_0, \theta^Y_{11})$ | $\Pr(\theta^X_1, \theta^Y_{11})$ | $\Pr(\theta^Y_{11})$ |
+------------------------------+----------------------------------+----------------------------------+----------------------+
| $\sum$                       | $\Pr(\theta^X_0)$                | $\Pr(\theta^X_1)$                | 1                    |
+------------------------------+----------------------------------+----------------------------------+----------------------+

: Nodal Types in $X \rightarrow Y$ Model. {tbl-colwidths="\[25,25,25,25\]"}

This table has 8 interior elements and so an unconstrained joint distribution would have $7$ degrees of freedom. A no confounding assumption means that $\Pr(\theta^X | \theta^Y) = \Pr(\theta^X)$, or $\Pr(\theta^X, \theta^Y) = \Pr(\theta^X)\Pr(\theta^Y)$. In this case we just put a distribution on the marginals and there would be $3$ degrees of freedom for $Y$ and $1$ for $X$, totaling $4$ rather than $7$.

```{r}
#| label: confound-params-df
#| echo: true
#| eval: false

confounded <- make_model("X -> Y ; X <-> Y")

confounded$parameters_df
```

```{r}
#| label: tbl-confound-params-df
#| tbl-cap: "Parameters Data Frame for Model with Confounding."
#| echo: false
#| eval: true

confounded <- make_model("X -> Y ; X <-> Y")

confounded$parameters_df |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("scale_down"))
```

@tbl-confound-params-df shows the parameters data frame produced by the code above. We see here that there are now two parameter families for parameters associated with the node $Y$. Each family captures the conditional distribution of $Y$'s nodal types, given $X$. For instance the parameter `Y01_X.1` can be interpreted as $\Pr(\theta^Y = \theta^Y_{01} | X=1)$.

To see exactly how the parameters map to causal types we can view the parameter matrix:

```{r}
#| label: confound-param-matrix
#| echo: true
#| eval: false

get_parameter_matrix(confounded)
```

```{r}
#| label: tbl-confound-param-matrix
#| tbl-cap: "Parameter Matrix for Model with Confounding."
#| echo: false
#| eval: true

get_parameter_matrix(confounded) |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("scale_down"))
```

The output is shown in @tbl-confound-param-matrix. Importantly, the $P$ matrix works as before, despite confounding. We can assess the probability of causal types by multiplying the probabilities of the constituent parameters.

-   Unlike nodal restrictions, a confounding relation can involve multiple nodal types simultaneously. For instance `make_model("X -> M -> Y") |> set_confound(list(X = "Y[X=1] > Y[X=0]"))` allows for a parameter that renders $X$ more or less likely depending on whether $X$ has a positive effect on $Y$ whether it runs through a positive or a negative effect on $M$.
-   The parameters needed to capture confounding relations depend on the direction of causal arrows. For example compare:
    -   `make_model("A -> W <- B ; A <-> W; B <-> W")$parameters_df |> dim` In this case we can decompose shocks on $A, B, W$ via: $\Pr(\theta^A, \theta^B, \theta^W) = \Pr(\theta^W | \theta^A, \theta^A)\Pr(\theta^A)\Pr(\theta^B)$, and we have 68 parameters.
    -   `make_model("A <- W -> B ; A <-> W; B <-> W")$parameters_df |> dim` In this case we have $\Pr(\theta^A, \theta^B, \theta^W) = \Pr(\theta^A | \theta^W)\Pr(\theta^B|\theta^W)\Pr(\theta^W)$ and just has just 18 parameters.

```{r}
#| label: fig-confound
#| echo: false
#| fig-cap: "Graph of Model with Confounding."
#| fig-pos: 't'
#| fig-align: center
#| out-width: "60%"

make_model("A <- X -> B; A <-> X; B <-> X") |> 
  plot()
```

#### Setting Priors {#priors}

Priors on model parameters can be added to the parameters data frame. The priors are interpreted as "alpha" arguments for a Dirichlet distribution. The Dirichlet distribution is a probability distribution over an $n-1$ dimensional unit simplex. It can be thought of as a generalization of the Beta distribution and is parameterized by an $n$-dimensional vector positive vector $\alpha$. Thus for example a Dirichlet with $\alpha = (1, 1, 1, 1, 1)$ gives a probability distribution over all non negative $5$-dimensional vectors that sum to $1$, e.g. $(0.1, 0.1, 0.1, 0.1, 0.6)$ or $(0.1, 0.2, 0.3, 0.3, 0.1)$. This particular value for $\alpha$ implies that all such vectors are equally likely. Other values for $\alpha$ can be used to control the expectation for each dimension as well as certainty. Thus for instance the vector $\alpha = (100, 1, 1, 1, 100)$ would result in more weight on distributions that are close to $(.5, 0, 0, 0. .5)$.

In `CausalQueries` priors are generally specified over the distribution of nodal types (or over the conditional distribution of nodal types, when there is confounding). Thus for instance in an $X \rightarrow Y$ model we have one Dirichlet distribution over the two types for $\theta^X$ and one Dirichlet distribution over the four types for $\theta^Y$.

To retrieve the model's priors we can run the following code:

```{r}
#| label: get-priors
#| echo: true
#| eval: true

make_model("X -> Y") |> get_priors()

```

Here the priors have not been specified and so they default to $1$, which corresponds to uniform priors. Alternatively you could set Jeffreys priors using `set_priors` as follows:

```{r}
#| label: set-priors
#| echo: true
#| eval: true

make_model("X -> Y") |> 
  set_priors(distribution = "jeffreys") |> 
  get_priors()

```

You can also add custom priors. Custom priors are most simply specified by being added as a vector of numbers using `set_priors`. For instance:

```{r}
#| label: set-priors-custom
#| echo: true
#| eval: true

make_model("X -> Y") |> 
  set_priors(1:6) |> 
  get_priors()
```

The priors here should be interpreted as indicating:

-   $\alpha_X = (1,2)$, which implies a distribution over $(\lambda^X_0, \lambda^X_1)$ centered on $(1/3, 2/3)$.
-   $\alpha_Y = (3,4,5,6)$, which implies a distribution over $(\lambda^Y_{00}, \lambda^Y_{10}, \lambda^Y_{01} \lambda^Y_{11})$ centered on $(3/18, 4/18, 5/18, 6/18)$.

For larger models it can be hard to provide priors as a vector of numbers. For that reason `set_priors` allows for more targeted modifications of the parameter vector. For instance:

```{r}
#| label: set-priors-statement
#| echo: true
#| eval: true

make_model("X -> Y") |>
  set_priors(statement = "Y[X=1] > Y[X=0]", alphas = 3) |>
  get_priors()
```

As setting priors simply requires mapping alpha values to parameters, the process boils down to selecting rows of the `parmeters_df` data frame, at which to alter values. When specifying a causal statement as above, `CausalQueries` internally identifies nodal types that are consistent with the statement, which in turn identify parameters to alter priors for. 

Note that there is nothing particularly unique about setting priors via causal syntax, as it is simply an abstract way of specifying how to subset the `parameters_df` data frame. We can achieve the same result as above by specifying nodal type for which we would like to adjust the priors:

```{r}
#| label: set-priors-nodal-type
#| echo: true
#| eval: true

make_model("X -> Y") |>
  set_priors(nodal_type = "01", alphas = 3) |>
  get_priors()
```

or even parameter names

```{r}
#| label: set-priors-param-names
#| echo: true
#| eval: true

make_model("X -> Y") |>
  set_priors(param_names = "Y.01", alphas = 3) |>
  get_priors()
```

As such `set_priors` allows for the specification of any non-redundant combination of arguments on the `param_names`, `node`, `nodal_type`, `param_set` and `given` columns of `parameters_df` to uniquely identify parameters to set priors for. Alternatively a fully formed subsetting statement may be supplied to `alter_at`. Since all these arguments get mapped to the parameters the identify internally they may be used interchangeably.[^3]

```{r}
#| label: set-priors-other
#| echo: true
#| eval: true

make_model("X -> M -> Y; X <-> Y") |>
  set_priors(node = "Y", 
             nodal_type = c("01","11"), 
             given = "X.1", 
             alphas = c(3,2)) |>
  get_priors()

make_model("X -> M -> Y; X <-> Y") |>
  set_priors(
    alter_at = 
      "node == 'Y' & nodal_type %in% c('01','11') & given == 'X.1'", 
    alphas = c(3,2)) |>
  get_priors()
```

It should be noted that while highly targeted prior setting is convenient and flexible; it should be done with caution. Setting priors on specific parameters in complex models; especially models involving confounding, may strongly affect inferences in intractable ways.

We additionally note that flat priors over nodal types do not necessarily translate into flat priors over queries. "Flat" priors over parameters in a parameter family put equal weight on each nodal type, but this in turn can translate into strong assumptions on causal quantities of interest.

For instance in an $X \rightarrow Y$ model in which negative effects are ruled out, the average causal effect implied by "flat" priors is $1/3$. This can be seen by querying the model as follows:

```{r}
#| label: set-priors-flat
#| echo: true
#| eval: false

make_model("X -> Y") |>
  set_restrictions(decreasing("X", "Y")) |>
  query_model("Y[X=1] - Y[X=0]", n_draws = 10000)

```

More subtly the *structure* of a model, coupled with flat priors, has substantive importance for priors on causal quantities. For instance with flat priors, priors on the probability that $X$ has a positive effect on $Y$ in the model $X \rightarrow Y$ is centered on $1/4$. But priors on the probability that $X$ has a positive effect on $Y$ in the model $X \rightarrow M \rightarrow Y$ is centered on $1/8$.

Again, you can use `query_model` to figure out what flat (or other) priors over parameters imply for priors over causal quantities:

```{r}
#| label: compare-flat-priors
#| echo: true
#| eval: false

make_model("X -> Y") |>
  query_model("Y[X=1] > Y[X=0]", n_draws = 10000)

make_model("X -> M -> Y") |>
  query_model("Y[X=1] > Y[X=0]", n_draws = 10000)

```

```{r}
#| label: tbl-compare-flat-priors
#| tbl-cap: "Comparison between $X \\rightarrow Y$ vs $X \\rightarrow M \\rightarrow Y$ Models."
#| tbl-subcap: 
#|    - "Average Causal Effect in $X \\rightarrow Y$ Model."
#|    - "Average Causal Effect in $X \\rightarrow M \\rightarrow Y$ Model."
#| echo: false
#| eval: false
#| include: false


make_model("X -> Y") |>
  query_model("Y[X=1] > Y[X=0]", n_draws = 10000) |> 
  dplyr::select(query, given, mean, sd, starts_with("cred")) |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("scale_down"))

make_model("X -> M -> Y") |>
  query_model("Y[X=1] > Y[X=0]", n_draws = 10000) |> 
  dplyr::select(query, given, mean, sd, starts_with("cred")) |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("scale_down"))
```

Caution regarding priors is particularly important when models are not identified, as is the case for many of the models considered here. In such cases, for some quantities, the marginal posterior distribution can be the same as the marginal prior distribution [@poirier1998revising].

The key point here is to make sure you do not fall into a trap of thinking that "uninformative" priors make no commitments regarding the values of causal quantities of interest. They do, and the implications of flat priors for causal quantities can depend on the structure of the model. Moreover for some inferences from causal models the priors can matter a lot even if you have a lot of data. In such cases it can be helpful to know what priors on parameters imply for priors on causal quantities of interest (by using `query_model`) and to assess how much conclusions depend on priors (by comparing results across models that vary in their priors).

#### Setting Parameters {#parameters}

By default, models have a vector of parameter values included in the `parameters_df` dataframe. These are useful for generating data, or for situations, such as process tracing, when one wants to make inferences about causal types ($\theta$), given case level data, under the assumption that the model is known.

The logic for setting parameters is similar to that for setting priors: effectively we need to place values on the probability of nodal types. The key difference is that whereas the $alpha$ value placed on a nodal types can be any positive number---capturing our certainty over the parameter value---the parameter values must lie in the unit interval, $[0,1]$. In general if parameter values are passed that do not lie in the unit interval, these are normalized so that they do.

Consider the causal model below. It has two parameter sets, X and Y, with six nodal types, two corresponding to X and four corresponding to Y. The key feature of the parameters is that they must sum to 1 within each parameter set.

```{r}
#| label: get-parameters
#| echo: true
#| eval: true

make_model("X -> Y") |> 
  get_parameters()

```

The example below illustrates a change in the value of the parameter $Y$ in the case it is increasing in $X$. Here nodal type `Y.Y01` is set to be .5, while the other nodal types of this parameter set were renormalized so that the parameters in the set still sum to one.

```{r}
#| label: set-parameters
#| echo: true
#| eval: true

make_model("X -> Y") |>
  set_parameters(statement = "Y[X=1] > Y[X=0]", parameters = .5) |>
  get_parameters()
```

<!-- # both setting priors and parameters offers a lot of options beyond causal statements now >> might be useful to show an example of a subset of those options at work for users to whom using causal statements is alien >> i.e. showcase the idea that under the hood a lot of this just comes down to filtering the parameters dataframe in various ways i.e. we have options: alter_at, node, nodal_type, param_set, given, statement, param_names -->

### Drawing and manipulating data

Once a model has been defined it is possible to simulate data from the model using the `make_data` function. This can be useful for instance for assessing the expected performance of a model given data drawn from some speculated set of parameter values.

```{r}
#| label: make-data-model
#| eval: true

model <- make_model("X -> M -> Y") 

```

#### Drawing data basics

By default, the parameters used are taken from `model$parameters_df`.

```{r}
#| label: make-data
#| echo: true
#| eval: true

sample_data_1 <- 
  model |> 
  make_data(n = 4)

```

However you can also specify parameters directly or use parameter draws from a prior or posterior distribution. For instance:

```{r}
#| label: make-data-draw
#| echo: true
#| eval: true

make_data(model, n = 3, param_type = "prior_draw")

```

Note that the data is returned ordered by data type as in the example above.

#### Drawing incomplete data

`CausalQueries` can be used in settings in which researchers have gathered different amounts of data for different nodes. For instance gathering $X$ and $Y$ data for all units but $M$ data only for some.

The function `make_data` allows you to draw data like this if you specify a data strategy indicating the probabilities of observing data on different nodes, possibly as a function of prior nodes observed.

```{r}
#| label: make-data-incomplete
#| echo: true
#| eval: true

sample_data_2 <-
  make_data(model,
            n = 8,
            nodes = list(c("X", "Y"), "M"),
            probs = list(1, .5),
            subsets = list(TRUE, "X==1 & Y==0"))

sample_data_2
```

#### Reshaping data

Whereas data naturally comes in long form, with a row per observation, as in the examples above, the data passed to `stan` is in a compact form, which records only the number of units of each data type. `CausalQueries` includes functions that lets you move between these two forms in case of need.

```{r}
#| label: collapse-data
#| echo: true
#| eval: true

sample_data_1 |> collapse_data(model)

sample_data_2 |> collapse_data(model)
```

In the same way it is possible to move from "compact data" to "long data" using `expand_data()`.

## Updating models {#sec-update}

The approach used by the `CausalQueries` package to updating parameter values given observed data uses `stan` and involves the following elements:

-   Dirichlet priors over parameters, $\lambda$ (which, in cases without confounding, correspond to nodal types)
-   A mapping from parameters to event probabilities, $w$
-   A likelihood function that assumes events are distributed according to a multinomial distribution given event probabilities.

We provide further details below.

### Data for stan

**GS: general edit to this section**

We use a generic `stan` model that works for all binary causal models. Rather than writing a new `stan` model for each causal model we send `stan` details of each particular causal model as data inputs.

In particular we provide a set of matrices that `stan` tailor itself to particular models: the parameter matrix ($P$ ) tells `stan` how many parameters there are, and how they map into causal types; an ambiguity matrix $A$ tells `stan` how causal types map into data types; and an event matrix $E$ relates data types into patterns of observed data (in cases where there are incomplete observations).

The internal function `prep_stan_data` prepares data for `stan`. You generally don't need to use this manually, but we show here a sample of what it produces as input for `stan`.

Note that NAs are interpreted as data not having been sought. So in this case the interpretation is that there are two data strategies: data on $Y$ and $X$ was sought in three cases; data on $Y$ only was sought in just one case.

<!-- # is it useful to touch on the possibility to update submodels when there is no confounding and no missing data here? -->

### How the stan model works

THIS DISCUSSION IS OUT OF DATE:

The `stan` model works as follows:

**MH: ADD DISCUSSION OF parmap;**

-   We are interested in "sets" of parameters. For example in the $X \rightarrow Y$ model we have two parameter sets (`param_sets`). The first is $\lambda^X \in \{\lambda^X_0, \lambda^X_1\}$ whose elements give the probability that $X$ is 0 or 1. These two probabilities sum to one. The second parameter set is $\lambda^Y \in \{\lambda^Y_{00}, \lambda^Y_{10}, \lambda^Y_{01} \lambda^Y_{11}\}$. These are also probabilities and their values sum to one. Note in all that we have 6 parameters but just 1 + 3 = 4 degrees of freedom.

-   We would like to express priors over these parameters using multiple Dirichlet distributions (two in this case). In practice because we are dealing with multiple simplices of varying length, it is easier to express priors over gamma distributions with a unit scale parameter and shape parameter corresponding to the Dirichlet priors, $\alpha$. We make use of the fact that $\lambda^X_0 \sim Gamma(\alpha^X_0,1)$ and $\lambda^X_1 \sim Gamma(\alpha^X_1,1)$ then $\frac{1}{\lambda^X_0 +\lambda^X_1}(\lambda^X_0, \lambda^X_1) \sim Dirichlet(\alpha^X_0, \alpha^X_1)$^[For a discussion of implementation of this approach in `stan` see discussion (here)[https://discourse.mc-stan.org/t/ragged-array-of-simplexes/1382].]

-   For any candidate parameter vector $\lambda$ we calculate the probability of *causal* types (`prob_of_types`) by taking, for each type $i$, the product of the probabilities of all parameters ($\lambda_j$) that appear in column $i$ of the parameter matrix $P$. Thus the probability of a $(X_0,Y_{00})$ case is just $\lambda^X_0 \times \lambda^Y_{00}$. The implementations in `stan` uses `prob_of_types_[i]` $= \prod_j \left(P_{j,i} \lambda_j + (1-P_{j,i})\right)$: this multiplies the probability of all parameters involved in the causal type (and substitutes 1s for parameters that are not). (`P` and `not_P` (1-$P$) are provided as data to `stan`).

-   The probability of data types, `w`, is given by summing up the probabilities of all causal types that produce a given data type. For example, the probability of a $X=0,Y=0$ case, $w_{00}$ is $\lambda^X_0\times \lambda^Y_{00} + \lambda^X_0\times \lambda^Y_{01}$. The ambiguity matrix $A$ is provided to `stan` to indicate which probabilities need to be summed.

-   In the case of incomplete data we first identify the set of "data strategies", where a collection of a data strategy might be of the form "gather data on $X$ and $M$, but not $Y$, for $n_1$ cases and gather data on $X$ and $Y$, but not $M$, for $n_2$ cases. The probability of an observed event, within a data strategy, is given by summing the probabilities of the types that could give rise to the incomplete data. For example $X$ is observed, but $Y$ is not, then the probability of $X=0, Y = \text{NA}$ is $w_{00} +w_{01}$. The matrix $E$ is passed to `stan` to figure out which event probabilities need to be combined for events with missing data.

-   The probability of a dataset is then given by a multinomial distribution with these event probabilities (or, in the case of incomplete data, the product of multinomials, one for each data strategy).

**GS: TO CHECK THE PATHS FUNCTIONALITY**

### Implementation

To update a `CausalQueries` model with data use:

```{r}
#| label: update-model
#| echo: true
#| eval: false

update_model(model, data)
```

where the data argument is a dataset containing some or all of the nodes in the model.

As `update_model()` calls `rstan::sampling` one can pass along all arguments in `...` to `rstan::sampling`. For instance:

-   `iter` sets the number of iterations and ultimately the number of draws in the posterior
-   `chains` sets the number of chains; doing multiple chains in parallel speeds things up

### Parallelization

If you have multiple cores you can do parallel processing by including this line before running `CausalQueries`:

```{r, eval = FALSE}
#| eval: false

library(parallel)

options(mc.cores = parallel::detectCores())

```

Additionally parallelizing across models or data while running MCMC chains in parallel can be achieved by setting up a nested parallel process. With 8 cores one can run 2 updating processes with 3 parallel chains each simultaneously. More generally the number of parallel processes at the upper level of the nested parallel structure are given by $\left \lfloor \cfrac{cores}{chains + 1} \right \rfloor$.

```{r}
#| eval: false

library(future)
library(future.apply)

chains <- 3
cores <- 8

future::plan(list(
      future::tweak(future::multisession, 
                    workers = floor(cores/(chains + 1))),
      future::tweak(future::multisession, 
                    workers = chains)
    ))

model <- make_model("X -> Y")
data <- list(data_1, data_2)

future.apply::future_lapply(data, function(d) {
  update_model(
    model = model,
    data = d,
    chains = chains,
    refresh = 0
  )
})

```

### Incomplete and censored data

`CausalQueries` assumes that missing data is missing at random, conditional on observed data. Thus for instance in a $X \rightarrow M \rightarrow Y$ model one might choose to observe $M$ in a random set of cases in which $X=1$ and $Y=1$. In that case if there are positive relations at each stage you may be more likely to observe $M$ in cases in which $M=1$. However observation of $M$ is still random *conditional* on the observed $X$ and $Y$ data. The `stan` model in `CausalQueries` takes account of this kind of sampling naturally by assessing the probability of observing a particular pattern of data within each data strategy. For a discussion see section 9.2.3.2 of @ii2023.

In addition it is possible to indicate when data has been censored and for the `stan` model to take this into account also. Say for instance that we only get to observe $X$ in cases where $X=1$ and not when $X=0$. This kind of sampling is non random conditional on observables. It is taken account however by indicating to `stan` that the probability of observing a particular data type is 0, regardless of parameter values. This is done using the `censored_types` argument in `update_model()`.

To illustrate, in the example below we observe perfectly correlated data for $X$ and $Y$. If we are aware that data in which $X \neg Y$ has been censored then when we update we do not move towards a belief that $X$ causes $Y$.

```{r}
list(uncensored = make_model("X -> Y") %>%
       update_model(data.frame(X=rep(0:1,5), Y=rep(0:1,5)),
                    refresh = 0, iter = 3000),
     censored = make_model("X -> Y") %>%
       update_model(data.frame(X=rep(0:1,5), Y=rep(0:1,5)),
                    censored_types = c("X1Y0",  "X0Y1"),
                    refresh = 0, iter = 3000)) %>%
  query_model(te("X", "Y"), using = "posteriors") |>
  select(model, query, mean, sd) |>
  kable(digits = 2)
```

### Output

The primary output from `update_model()` is a posterior distribution over model parameters, stored as a dataframe in `model$posterior_distribution`. However another of other objects are also optionally stored:

Models can be queried using the `query_distribution` and `query_model` functions. The difference between these functions is that `query_distribution` examines a single query and returns a full distribution of draws from the distribution of the estimand (prior or posterior); `query_model` takes a collection of queries and returns a dataframe with summary statistics on the queries.

## Queries  {#sec-query}

### Calcuating factual and counterfactual quantities

We can map from causal types to data types by propagating realized values on nodes forward in the DAG, moving from exogenous or intervened upon nodes to their descendants in generational order. The `realise_outcomes` function achieves this by traversing the DAG, while recording for each node's nodal types, the values implied by realizations on the node's parents. By way of example, consider the first causal type of a $X \rightarrow Y$ model:

1.  $X$ is exogenous and has a realized value of $0$
2.  We substitute for $Y$ the value implied by the $00$ nodal type given a $0$ value on $X$, which in turn is $0$ (see nodal types)

Recovering implied values on complex nodal types efficiently at scale, exploits the fact that nodal types are the Cartesian Product of possible parent realizations. Finding the index of a Node's realized value in a nodal type given parent realizations is equivalent to finding the row index in the Cartesian Product matrix corresponding to those parent realizations. By definition of the Cartesian product, the number of consecutive 0 or 1 elements in a given column is $2^{columnindex}$, when indexing columns from 0. Given a set of parent realizations $R$ indexed from 0, the corresponding row index in the Cartesian Product Matrix indexed from 0 can thus be computed via: $row = (\sum_{i = 0}^{|R| - 1} (2^{i} \times R_i))$.

Calling `realise_outcomes` on the above model thus yields:

```{r}
#| label: realise-outcomes
#| echo: true

make_model("X -> Y") |> realise_outcomes()

```

with row names indicating nodal types and columns realized values. Intervening on $X$ with $do(X=1)$ yields:

```{r}
#| label: realise-outcomes-do
#| echo: true

make_model("X -> Y") |> realise_outcomes(dos = list(X = 1))

```

<!-- # should we show data types for comparison here with all_data_types() ? -->

### Causal Syntax {#sec-syntax}

Scholars typically explore a broad range of causal questions. `CausalQueries` provides syntax for the formulation of various causal queries, ranging from relatively straightforward queries, such as  the proportion of units where $Y$ equals 1 (expressed as `"Y == 1"`), to more detailed ones, such as  questions about the types for which $Y$ is greater when $X$ equals 1 than when $X$ equals 0 (expressed as `"Y[X=1] > Y[X=0]"`). 


This syntax enables users to write arbitrary causal queries to interrogate their models. For factual queries, users may employ logical statements to ask questions about observed conditions. Take, for example, the query mentioned above about the proportion of units where $Y$ equals 1, expressed as `"Y == 1"`.  In this case the logical operator  ==  indicates that `CausalQueries` should consider units that fulfill the condition of strict equality where $Y$ equals 1.^[`CausalQueries` also accepts = as a shorthand for ==. However, == is preferred as it is the conventional logical operator to express a condition of strict equality.]

Counterfactual queries can be expressed in `CausalQueries` by using square brackets \[\]. Inside the brackets, variables that are to be intervened are specified. For instance, consider the counterfactual query that asks about the types for which $Y$ equals 1 when $X$ is _set to_ 0. In this case, since $X$ is being intervened to be zero, $X$ is placed inside the brackets. Given that $Y$ equaling 1 is a factual condition, it is expressed as in the paragraph above using the logical operator `==`. More precisely, such query can be written as `"Y[X=0] == 1"`.

With the aid of logical operators such as `==`, `!=`, `>`, `>=` `<`,`<=` , and the use of square brackets, users can formulate a broad range of queries. For example, they can make queries about cases where $X$ has a positive effect on $Y$, i.e., whether $Y$ is greater when $X$ is set to 1 compared to when $X$ is set to 0, expressed as `"Y[X=1] > Y[X=0]"`. Alternatively, they can explore complex counterfactuals like `"Y[M=M[X=0], X=1]==1"`. In this case, `CausalQueries` looks for the types for which $Y$ equals 1 when $X$ is set to 1, while keeping $M$ constant at the value it would take if $X$ were 0.


One way to develop a clearer understanding of what types are being targeted with each query is using the function  `get_query_types`. For instance, consider the model $X \rightarrow Y \leftarrow M$ and the query `"Y[X=1, M=1] > Y[X=0, M=0]"`.

In this case, the aim is to identify the types for which $Y$ equals zero when $X = 0$ and  $M = 0$, and $Y$ equals 1 when $X = 1$ and $M = 1$. The specific values that Y takes for other combinations of $X$ and $M$ are not relevant for this query. Therefore, the types targeted in this query have a zero as the first digit of the subscript i.e., when $X = 0$ and $ M = 0$, and a 1 as the last digit i.e., when $X = 1$ and $M = 1$, and any value in the second and third digits. The code and output below offer a more precise representation of this.

```{r}
#| eval: true
#| echo: true

make_model('X -> M -> Y <- X') |> 
  get_query_types(query = "Y[X=1, M=1] > Y[X=0, M=0]", 
                  map = "nodal_type")
```




**LM: REVIEW AND FIX SECTION**

WRITE SECTION DESCRIBING FACTUAL AND COUNTERFACTULA QUERIES AND THE SYNTAX INCLUDING Role of \[\] AND OF GIVEN

`get_query_types`

return sets; discuss how all queries related to these sets



start off with query Y==1 then Y\[X=1\] ==1 then Y\[X=1\] \>= Y\[X=0\] then show linear functions possible Y\[X=1\] - Y\[X=0\]

### Query implementation

### Queries by hand

Queries can draw directly from the prior distribution of the posterior distribution provided by `stan`.

<!-- GS: Can we avoid using fabricate here? -->

```{r}
#| eval: false

library(DeclareDesign)

data  <- 
  fabricate(N = 100, X = complete_ra(N), Y = X)

model <- 
  make_model("X -> Y") |>
  update_model(data, iter  = 4000)

model$posterior_distribution |> 
  ggplot(aes(Y.01 - Y.10)) + geom_histogram()

```

```{r}
#| label: fig-posterior-dist
#| fig-cap: "Posterior on 'Probability $Y$ is increasing in $X$'"
#| fig-pos: "t"
#| fig-align: center
#| out-width: "60%"
#| echo: false

if (params$run) {

  data  <- fabricate(N = 100, X = complete_ra(N), Y = X)

  make_model("X -> Y") |>
    update_model(data, iter  = 4000, refresh = 0) |>
    write_rds("saved/app_2_illus.rds")

}

model <- read_rds("saved/app_2_illus.rds")

model$posterior_distribution |> 
  ggplot(aes(Y.01 - Y.10)) + geom_histogram() + theme_bw()

```

\FloatBarrier

### Query distribution

`query_distribution` works similarly except that distributions over queries are returned:

```{r}
#| label: fig-query-dist
#| fig-cap: "Prior on 'Probability $Y$ is increasing in $X$'"
#| fig-pos: "t"
#| fig-align: center
#| out-width: "60%"
#| external: true

make_model("X -> Y") |> 
  query_distribution(
    query = list(increasing = "(Y[X=1] > Y[X=0])"), 
    using = "priors") |>
  ggplot(aes(increasing)) + geom_histogram() + theme_bw()

```

### Case level queries

Sometimes one is interested in assessing the value of a query for a *particular case*. In a sense this is equivalent to posing a conditional query, querying conditional on values in a case. For instance we might consult our posterior lipds model and ask about the effect of $X$ on $Y$ for a case in which $Z=1, X=1$ and $Y=1$.

```{r}
#| label: case-level-query
#| echo: true
#| eval: false

lipids_model |>
    query_model(query = "Y[X=1] - Y[X=0]",
              given = c("X==1 & Y==1 & Z==1"),
              using = "posteriors")
```

```{r}
#| label: tbl-case-level-query
#| tbl-cap: "Case Level Quiry Example."
#| echo: false
#| eval: true

lipids_model |>
  query_model(query = "Y[X=1] - Y[X=0]",
              given = c("X==1 & Y==1 & Z==1"),
              using = "posteriors") |> 
  dplyr::select(query, given, mean, sd, starts_with("cred")) |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("scale_down", "hold_position"))
```

The answer we get in @tbl-case-level-query is what we now believe for all cases in which $Z=1, X=1$ and $Y=1$. It is in fact the expected average effect among cases with this data type and so this expectation has an uncertainty attached to it.

Subtly though this is, in principle, different to what we would infer for a "new case" that we wonder about. When inquirying about a new case, the case level query *updates* on the given information observed in the new case. The resulting inference is different to the inference that would be made from the posterior *given* the features of the case.

To illustrate, consider a model for which we are quite sure that $X$ causes $Y$ but we do not know whether it works through two positive effects or two negative effects.

Thus we do not know if M=0 would suggest an effect or no effect. If asked what we would infer for a case that had $M=0$ we would not know whether $M=0$ information is consistent with a positive effect or not. However if provided with a randomly case and learn that it has $M=0$, then we update about the causal model and infer that there is an effect in this case (but that there wouldn't be were $M=1$).

```{r}
#| echo: true
#| eval: false

# DO ALL THIS IN ONE TABLE

set.seed(1)
 model <-
   make_model("X -> M -> Y") |>
   update_model(data.frame(X = rep(0:1, 8), Y = rep(0:1, 8)), iter = 10000)

 Q <- "Y[X=1] > Y[X=0]"
 G <- "X==1 & Y==1 & M==1"
 QG <- "(Y[X=1] > Y[X=0]) & (X==1 & Y==1 & M==1)"

 # In this case these are very different:
 query_distribution(model, Q, given = G, using = "posteriors")[[1]] |> mean()
 query_distribution(model, Q, given = G, using = "posteriors",
   case_level = TRUE)

 # These are equivalent:
 # 1. Case level query via function
 query_distribution(model, Q, given = G,
    using = "posteriors", case_level = TRUE)

 # 2. Case level query by hand using Bayes
 distribution <- query_distribution(
    model, list(QG = QG, G = G), using = "posteriors")

 mean(distribution$QG)/mean(distribution$G)
```

### Batch queries

The `query_model()` function takes causal queries and conditions (`given`) and specifies the parameters to be used. The result is a data frame which can be displayed as a table.

```{r}
#| label: batch-query
#| echo: true
#| eval: false

models <- list(
 `1` = make_model("X -> Y"),
 `2` = make_model("X -> Y") |> set_restrictions("Y[X=1] < Y[X=0]")
 )

query_model(
  models,
  query = list(ATE = "Y[X=1] - Y[X=0]", 
               POS = "Y[X=1] > Y[X=0]"),
  given = c(TRUE,  "Y==1 & X==1"),
  case_level = c(FALSE, TRUE),
  using = c("parameters", "priors"),
  expand_grid = TRUE)
```

```{r}
#| label: tbl-batch-query
#| tbl-cap: "Results for Two Queries on Two Models."
#| echo: false
#| eval: true

models <- list(
 `1` = make_model("X -> Y"),
 `2` = make_model("X -> Y") |> set_restrictions("Y[X=1] < Y[X=0]")
 )

query_model(
  models,
  query = list(ATE = "Y[X=1] - Y[X=0]", 
               POS = "Y[X=1] > Y[X=0]"),
  given = c(TRUE,  "Y==1 & X==1"),
  case_level = c(FALSE, TRUE),
  using = c("parameters", "priors"),
  expand_grid = TRUE) |>
  dplyr::select(-starts_with("cred")) |> 
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE,
    longtable = TRUE,
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("hold_position"))

```

```{=tex}
\FloatBarrier
```

## Computational details and software requirements {.unnumbered}

+------------------------:+:----------------------------------------------------------------------------------:+:-----------+
| Version                 | -   1.0.1                                                                          |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| Availability            | -   Stable Release: https://cran.rstudio.com/web/packages/CausalQueries/index.html |            |
|                         | -   Development: https://github.com/integrated-inferences/CausalQueries            |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| Issues                  | -   https://github.com/integrated-inferences/CausalQueries/issues                  |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| Operating Systems       | -   Linux                                                                          |            |
|                         | -   MacOS                                                                          |            |
|                         | -   Windows                                                                        |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| Testing Environments OS | -   Ubuntu 22.04.2                                                                 |            |
|                         | -   Debian 12.2                                                                    |            |
|                         | -   MacOS                                                                          |            |
|                         | -   Windows                                                                        |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| Testing Environments R  | -   R 4.3.1                                                                        |            |
|                         | -   R 4.3.0                                                                        |            |
|                         | -   R 4.2.3                                                                        |            |
|                         | -   r-devel                                                                        |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| R Version               | -   R(\>= 3.4.0)                                                                   |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| Compiler                | -   either of the below or similar:                                                |            |
|                         | -   g++                                                                            |            |
|                         | -   clang++                                                                        |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| Stan requirements       | -   inline                                                                         |            |
|                         | -   Rcpp (\>= 0.12.0)                                                              |            |
|                         | -   RcppEigen (\>= 0.3.3.3.0)                                                      |            |
|                         | -   RcppArmadillo (\>= 0.12.6.4.0)                                                 |            |
|                         | -   RcppParallel (\>= 5.1.4)                                                       |            |
|                         | -   BH (\>= 1.66.0)                                                                |            |
|                         | -   StanHeaders (\>= 2.26.0)                                                       |            |
|                         | -   rstan (\>= 2.26.0)                                                             |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| R-Packages Depends      | -   dplyr                                                                          |            |
|                         | -   methods                                                                        |            |
+-------------------------+------------------------------------------------------------------------------------+------------+
| R-Packages Imports      | -   dagitty (\>= 0.3-1)                                                            |            |
|                         | -   dirmult (\>= 0.1.3-4)                                                          |            |
|                         | -   stats (\>= 4.1.1)                                                              |            |
|                         | -   rlang (\>= 0.2.0)                                                              |            |
|                         | -   rstan (\>= 2.26.0)                                                             |            |
|                         | -   rstantools (\>= 2.0.0)                                                         |            |
|                         | -   stringr (\>= 1.4.0)                                                            |            |
|                         | -   ggdag (\>= 0.2.4)                                                              |            |
|                         | -   latex2exp (\>= 0.9.4)                                                          |            |
|                         | -   ggplot2 (\>= 3.3.5)                                                            |            |
|                         | -   lifecycle (\>= 1.0.1)                                                          |            |
+-------------------------+------------------------------------------------------------------------------------+------------+

: {tbl-colwidths=\[30,70\]}

The results in this paper were obtained using [R]{.proglang}\~3.4.1 with the [MASS]{.pkg}\~7.3.47 package. [R]{.proglang} itself and all packages used are available from the Comprehensive [R]{.proglang} Archive Network (CRAN) at \[https://CRAN.R-project.org/\].

## Acknowledgments {.unnumbered}

::: callout
The approach to generating a generic stan function that can take data from arbitrary models was developed in key contributions by [Jasper Cooper](http://jasper-cooper.com/) and [Georgiy Syunyaev](http://gsyunyaev.com/). [Lily Medina](https://lilymedina.github.io/) did magical work pulling it all together and developing approaches to characterizing confounding and defining estimands. Clara Bicalho helped figure out the syntax for causal statements. Julio Solis made many key contributions figuring out how to simplify the specification of priors. Merlin Heidemanns figured out the `rstantools` integration and made myriad code improvements. Till Tietz revamped the entire package and improved every part of it.
:::

## References {.unnumbered}

::: {#refs}
:::

{{< pagebreak >}}

## More technical details {#sec-techdetails .unnumbered}

::: callout
Appendices can be included after the bibliography (with a page break). Each section within the appendix should have a proper section title (rather than just *Appendix*).

For more technical style details, please check out JSS's style FAQ at \[https://www.jstatsoft.org/pages/view/style#frequently-asked-questions\] which includes the following topics:

-   Title vs. sentence case.
-   Graphics formatting.
-   Naming conventions.
-   Turning JSS manuscripts into [R]{.proglang} package vignettes.
-   Trouble shooting.
-   Many other potentially helpful details...
:::

## Using BibTeX {#sec-bibtex .unnumbered}

::: callout
References need to be provided in a {{< bibtex >}} file (`.bib`). All references should be made with `@cite` syntax. This commands yield different formats of author-year citations and allow to include additional details (e.g.,pages, chapters, \dots) in brackets. In case you are not familiar with these commands see the JSS style FAQ for details.

Cleaning up {{< bibtex >}} files is a somewhat tedious task -- especially when acquiring the entries automatically from mixed online sources. However, it is important that informations are complete and presented in a consistent style to avoid confusions. JSS requires the following format.

-   item JSS-specific markup (`\proglang`, `\pkg`, `\code`) should be used in the references.
-   item Titles should be in title case.
-   item Journal titles should not be abbreviated and in title case.
-   item DOIs should be included where available.
-   item Software should be properly cited as well. For [R]{.proglang} packages `citation("pkgname")` typically provides a good starting point.
:::

## Appendix: stan code

`prep_stan_data` then returns a list of objects that `stan` expects to receive. These include indicators to figure out where a parameter set starts (`l_starts`, `l_ends`) and ends and where a data strategy starts and ends (`strategy_starts`, `strategy_ends`), as well as the matrices described above.

```{r ch2prep, comment = "", include = FALSE}

#CausalQueries:::prep_stan_data(model, compact_data)

```

MOVE TO APPENDIX? ADD DISCUSSION OF PARMAP

Below we show the `stan` code. This starts off with a block saying what input data is to be expected. Then there is a characterization of parameters and the transformed parameters. Then the likelihoods and priors are provided. `stan` takes it from there and generates a posterior distribution.

```{r}
#| echo: false
#| results: markup

if (params$run) {
  
  make_model("X -> Y") |> 
    update_model(
      data = data,
      keep_fit = TRUE, 
      refresh = 0) |> 
    (\(.) .$stan_objects$stan_fit)() |>
    write_rds("saved/fit.rds")
  
}

cat(rstan::get_stancode(read_rds("saved/fit.rds")))

```

[^1]: `CausalQueries` can be used also to analyse non binary data though with a cost of greatly increased complexity. See section 9.4.1 of @ii2023 for an approach that codes non binary data as a profile of outcomes on multiple binary nodes.

[^2]: See (Using BibTeX)\[#sec-bibtex\] for more details.

[^3]: See `?set_priors` and `?make_priors` for many more examples.
