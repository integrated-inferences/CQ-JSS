---
title: "Making, Updating, and Querying Causal Models using `CausalQueries`"
abstract: "The [R]{.proglang} package `CausalQueries`  can be used to make, update, and query causal models defined on binary nodes. Users provide a causal statement of the form `X -> M <- Y; M <-> Y` which is interpreted as a structural causal model over a collection of binary nodes.  Then `CausalQueries` allows users to (1) identify the set of principal strata---causal types---required to characterize all possible causal relations between nodes consistent with the causal statement (2) determine a set of parameters needed to characterize distributions over these types (3) update beliefs over distributions of causal types, using a `stan` model plus data and (4) pose a wide range of causal queries of the model, using either the prior distribution, the posterior distribution, or a user-specified candidate vector of parameters."
keywords-formatted: [causal model, Bayesian updating, DAG, Stan]
author:
  - name: Till Tietz
    affiliations:
      - name: WZB
        department: IPI
        address: Reichpietschufer 50
        city: Berlin
        country: Germany
        postal-code: 10785
    email: ttietz2014@gmail.com
    orcid: 0000-0002-2916-9059
    url: https://github.com/till-tietz
  - name: Lily Medina
    affiliations:
      - name: University of California, Berkeley
    orcid: 0009-0004-2423-524X
    email: lily.medina@berkeley.edu
    url: https:/lilymedina.github.io/
  - name: Georgiy Syunyaev
    affiliations:
      - name: Vanderbilt University
      - name: WZB
    orcid: 0000-0002-4391-6313
    email: g.syunyaev@vanderbilt.edu
    url: https://gsyunyaev.com/
  - name: Macartan Humphreys
    affiliations:
      - name: WZB
        department: IPI
        address: Reichpietschufer 50
        city: Berlin
        country: Germany
        postal-code: 10785
    orcid: 0000-0001-7029-2326
    email: macartan.humphreys@wzb.eu
    url: https://macartan.github.io/
format:
  jss-pdf:
    keep-tex: true
    include-in-header: supp/in-header.tex
    tbl-cap-location: top
    knitr: 
      opts_chunk:
        prompt: true
        comment: "#>"
        dev: "tikz"
bibliography: supp/cq_jss.bib
fontsize: 11pt
execute: 
  eval: true
  warning: false
  error: false
  cache: false
---

```{r}
#| label: preamble
#| include: false

# knitr::purl("paper.qmd")
# knitr::spin("submission/code.R")
library(tidyverse)
library(CausalQueries)
library(microbenchmark)
library(parallel)
library(future)
library(future.apply)
library(knitr)
library(rstan)

library(tikzDevice)

options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)
options(mc.cores = parallel::detectCores())


set.seed(1, "L'Ecuyer-CMRG")
theme_set(theme_bw())

```

## Introduction: Causal models {#sec-intro}

`CausalQueries` is an [R]{.proglang} package that lets users make, update, and query causal models. Users provide a statement that reports a set of binary variables and the relations of "causal ancestry" between them, that is, a statement indicating which variables are direct causes of other variables, given the other variables in the model. Once such a statement is provided to `make_model()`, `CausalQueries` generates a parameter vector that fully describes a probability distribution over all possible types of causal relations between variables ("causal types"). Given a prior over parameters and data over some or all nodes, `update_model()` deploys a Stan [@carpenter_stan_2017] model to generate a posterior distribution over causal models. The function `query_model()` can then be used to ask a wide range of causal queries of the model, using either the prior distribution, the posterior distribution, or a user-specified candidate vector of parameters.

In the next section we provide a motivating example. We then describe how the package relates to existing available software. @sec-theory gives an overview of the statistical model behind the package. @sec-make, @sec-update, and @sec-query then describe the main functionality for the major operations using the package. We provide further computation details in the final section.

## Motivating example

Before providing details on package functionality we illustrate these three core functions by showing how to use `CausalQueries` to replicate the analysis in [@chickering_clinicians_1996; see also @humphreys_integrated_2023]. @chickering_clinicians_1996 seek to draw inferences on causal effects in the presence of imperfect compliance. We have access to an instrument $Z$ (a randomly assigned prescription for cholesterol medication), which is a cause of $X$ (treatment uptake) but otherwise unrelated to $Y$ (cholesterol). We imagine we are interested in three specific queries. The first is the average causal effect of $X$ on $Y$. The second is the average effect for units for which $X=0$ and $Y=0$. The last is the average effect for "compliers": units for which $X$ responds positively to $Z$. Thus two of these queries are conditional queries, with one conditional on a counterfactual quantity.

The data on $Z$, $X$, and $Y$ is given in @chickering_clinicians_1996 but also included in `CausalQueries`. The data is complete for all units and looks, in "compact form,"[^1] as follows:

```{r}
#| echo: true

data("lipids_data")

lipids_data

```

[^1]: Note that in compact form we simply record the number of units ("count") that display each possible pattern of outcomes on the three variables ("event"). The "strategy" column records the set of variables for which data has been recorded. In this illustration the data is complete and so the implied strategy is `ZXY` for all units.

Then `CausalQueries` allows you to create the model, input data, and update the model as follows:

```{r}
#| echo: true
#| eval: true
#| purl: true
#| message: false

lipids_model <-  
  make_model("Z -> X -> Y; X <-> Y") |>
  update_model(lipids_data)

```

Finally, the model can then be queried:

```{r}
#| echo: true
#| eval: true
#| purl: true

lipids_queries <- 
  lipids_model  |>
  query_model(query = "Y[X=1] - Y[X=0]",
              given = c("All",  "X==0 & Y==0", "X[Z=1] > X[Z=0]"),
              using = "posteriors") 

```

The output is a data frame with estimates, posterior standard deviations, and credibility intervals. @tbl-lipids shows the output from the analysis of the lipids data. Rows 1 and 2 in the table replicate results in @chickering_clinicians_1996; row 3 returns inferences for complier average effects.

```{r}
#| label: tbl-lipids
#| tbl-cap: "Replication of \\citet{chickering_clinicians_1996}."
#| echo: false

lipids_queries |>
  dplyr::select(query, given, mean, sd, starts_with("cred")) |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE, 
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("scale_down"))

```

As we describe below, the same basic procedure of making, updating, and querying models, can be used (up to computational constraints) for arbitrary causal models, for different types of data structures, and for all causal queries that can be posed of the causal model.

## Connections to existing packages

The literature on causal inference and its software ecosystem is large, spanning the social and natural sciences as well as computer science and applied mathematics. Here we contextualize the scope and functionality of `CausalQueries` within the subset of the causal inference domain addressing the evaluation of causal queries on causal models encoded as directed acyclic graphs (DAGs) or structural equation models (SEMs). @tbl-software provides an overview of relevant software and discusses key connections, advantages and disadvantages with respect to `CausalQueries`.

+--------------+--------------------------+---------------------+--------------+---------------------------------------------------------------------+
| Software     | Source                   | Language            | Availability | Scope                                                               |
+==============+==========================+=====================+==============+=====================================================================+
| `causalnex`  | @beaumont_causalnex_2021 | [Python]{.proglang} | -   pip      | -   causal structure learning                                       |
|              |                          |                     |              | -   querying marginal distributions                                 |
|              |                          |                     |              | -   discrete data                                                   |
+--------------+--------------------------+---------------------+--------------+---------------------------------------------------------------------+
| `pclag`      | @kalisch_causal_2012     | [R]{.proglang}      | -   CRAN     | -   causal structure learning                                       |
|              |                          |                     | -   GitHub   | -   ATEs under linear conditional expectations, no hidden selection |
+--------------+--------------------------+---------------------+--------------+---------------------------------------------------------------------+
| `DoWhy`      | @dowhy                   | [Python]{.proglang} | -   pip      | -   identification                                                  |
|              |                          |                     |              | -   average and conditional causal effects                          |
|              |                          |                     |              | -   robustness checks                                               |
+--------------+--------------------------+---------------------+--------------+---------------------------------------------------------------------+
| `autobounds` | @duarte_automated_2023   | [Python]{.proglang} | -   Docker   | -   bounding causal effects                                         |
|              |                          |                     | -   GitHub   | -   partial identification                                          |
|              |                          |                     |              | -   DAG canonicalization                                            |
|              |                          |                     |              | -   binary data                                                     |
+--------------+--------------------------+---------------------+--------------+---------------------------------------------------------------------+
| `causaloptim`| @sachs_general_2023      | [R]{.proglang}      | -   CRAN     | -   bounding causal effects                                         |
|              |                          |                     | -   GitHub   | -   non-identified queries                                          |
|              |                          |                     |              | -   binary data                                                     |
+--------------+--------------------------+---------------------+--------------+---------------------------------------------------------------------+

: Related software. {#tbl-software tbl-colwidths="\[19,20,10,18,38\]"}

`causalnex` is a highly comprehensive software in the causal modeling domain, offering a suite of functions rich in features and optimized for the learning, updating, and querying of causal models using discrete data. Its avoidance of the intricate model parametrization, characterized by principal strata (nodal types), enables `causalnex` to adeptly process non-binary data and scale to large causal models. This approach; however, significantly constrains the variety of feasible queries and the extent of prior knowledge that can be incorporated into models. In this capacity, `causalnex` mirrors machine learning strategies in causal inference, prioritizing the learning of causal structures in environments abundant with variables yet potentially deficient in domain-specific knowledge; thus focusing on the assessment of basic queries over marginal distributions in learned DAGs. Conversely, the complex model structure utilized by `CausalQueries` is particularly advantageous for causal queries in settings where domain knowledge is more prevalent.

Like `causalnex`, `pclag` places particular emphasis on causal structure learning, utilizing the resultant DAGs to recover average treatment effects (ATEs) across all learned Markov-equivalent classes implied by observed data that satisfy linearity of conditional expectations. This approach again is more restrictive than `CausalQueries` in the DAGs and particularly the queries it allows.

`DoWhy` is a feature-rich, mature inference framework that emphasizes causal identification, causal effect estimation, and assumption validation. Given a user-specified DAG, it deploys do-calculus to find expressions that identify desired causal-effects via Back-door, Front-door, IV and mediation identification criteria and leverages the identified expression and standard estimators to estimate the desired estimand. Following estimation, `DoWhy` deploys a comprehensive refutation engine implementing a large set of robustness tests. While this approach allows it to efficiently handle varied data types on large causal models, the decision to not parameterize the DAG itself places substantial limitations on the types of queries that can be posed.

The software bearing the highest resemblance to `CausalQueries` for model definition are `autobounds` and `causaloptim`. Dealing with binary causal models, their definitions of principal strata (nodal types) and the resultant set of causal relations on the DAG (causal types) are very close to those of `CausalQueries`. Differences in model definition arise for disturbance terms and confounding being defined implicitly via main nodes and edges in `CausalQueries` vs explicitly via separate disturbance nodes in `autobounds` and `causaloptim`. While `CausalQueries` assumes canonical form for input DAGs, `autobounds` and `causaloptim` facilitate canonicalization. The essential difference between the methods; however, lies in their approach to evaluating queries.

Both `autobounds` and `causaloptim` build on seminal approaches in @balke_bounds_1997 to construct bounds of queries, using constrained polynomial and linear optimization respectively. In contrast, `CausalQueries` utilizes Bayesian inference to generate a posterior over the causal model which is then queried [consistent with @chickering_clinicians_1996; @zhang_partial_2022]. A key difference is the target of inference. The polynomial and linear programming approach to querying is in principle suited to handling larger causal models, though given their similarity in model parametrization, `autobounds`, `causaloptim` and `CausalQueries` face similar constraints induced by parameter spaces expanding rapidly with model size. The Bayesian approach to model updating and querying holds the efficiency advantage that a model can be updated once and queried arbitrarily, while expensive optimization runs are required for each separate query in `autobounds` and `causaloptim`.

```{r}
#| echo: true
#| results: markup
#| eval: false
#| include: false
#| purl: true

# Calculation of parameter lengths cited in text

make_model("A -> B -> C -> D -> E") |>
  grab("parameters_df") |> 
  nrow()

make_model("A -> E <- B; C-> E <- D") |>
  grab("parameters_df") |> 
  nrow()
```

Summarizing, the particular strength of `CausalQueries` is to allow users to specify arbitrary DAGs, to specify arbitrary queries defined on the DAG, and use the same canonical procedure to form Bayesian posteriors over those queries whether or not the queries are identified. Thus, for example, if researchers are interested in learning about a quantity like the local average treatment effect and their model satisfies the conditions in @angrist_identification_1996, then updating will recover valid estimates as data grows even if researchers are unaware that the local average treatment effect is identified and are ignorant of the estimation procedure proposed by @angrist_identification_1996.

There are two broad limitations on the sets of models handled natively by `CausalQueries`. First `CausalQueries` is designed for models with a relatively small number of binary nodes. Because there is no compromise made on the space of possible causal relations implied by a given model, the parameter space grows very rapidly with the complexity of the causal model. The complexity also depends on the causal structure and grows rapidly with the number of parents affecting a given child. A chain model of the form $A \rightarrow B \rightarrow C \rightarrow D \rightarrow E$ has just 40 parameters. A model in which $A, B, C, D$ are all direct ancestors of $E$ has $65,544$ parameters. Moving from binary to nonbinary nodes has similar effects. The restriction to binary nodes is for computational and not conceptual reasons. In fact, it is possible to employ `CausalQueries` to answer queries from models with non-binary nodes but in general, the computational complexity makes analysis of these models prohibitively costly.[^2]

[^2]: For more on computation constraints and strategies to update and query large models see the associated package `CausalQueriesTools` available via `devtools::install_github("till-tietz/CausalQueriesTools")`. The core approach used here is to divide large causal models into modules, update on modules and reassemble to pose queries. Also, see section 9.4.1 of @humphreys_integrated_2023 for an approach that codes non-binary data as a profile of outcomes on multiple binary nodes.

Second, the package is geared towards learning about populations from independently sampled units from populations. Thus the basic setup does not address problems of clustering, hierarchical structures, or purposive sampling. The broader framework can however be used for these purposes [see section 9.4 of @humphreys_integrated_2023]. The targets of inference are usually case-level quantities or population quantities and `CausalQueries` is not well suited for estimating sample quantities.

## Statistical model {#sec-theory}

The core conceptual framework is described in Pearl's *Causality* [@pearl_causality_2009] but can be summarized as follows [using the notation proposed in @humphreys_integrated_2023]:

```{=tex}
\begin{definition}
  
  A ``\textbf{causal model}'' is:
  \begin{enumerate}
    \item an ordered collection of ``endogenous nodes" $Y = \{Y_1, Y_2, \dots, Y_n\}$
    \item an ordered collection of ``exogenous nodes" $\Theta = \{\theta^{Y_1}, \theta^{Y_1}, \dots, \theta^{Y_n}\}$
    \item a collection of functions $F = \{f_{Y_1}, f_{Y_2}, \dots, f_{Y_n}\}$ specifying, for each $j$, how outcome $y_j$ depends on $\theta_j$ and realizations of endogenous nodes prior to $j$.
    \item a probability distribution over $\Theta$, $\lambda$.
  \end{enumerate}
  
\end{definition}
```
By default, `CausalQueries` assumes endogenous nodes to be binary. When we specify a causal structure we specify which endogenous nodes are (possibly) direct causes of a node, $Y_j$, given other nodes in the model. These nodes are called the parents of $Y_j$, $PA_j$ (we use upper case $PA_j$ to indicate the collection of nodes and lower case $pa_j$ to indicate a particular set of values that these nodes might take on). With discrete valued nodes, it is possible to identify all possible ways that a node might respond to its parents. We refer to the ways that a node responds as "nodal type." The set of nodal types corresponds to principal strata familiar, for instance, in the study of instrumental variables [@frangakis_principal_2002].

If node $Y_i$ can take on $k_i$ possible values then the set of possible values that can be taken on by parents of $j$ is $m_j :=\prod_{i\in PA_j}k_i$. Then there are $k_j^{m_j}$ different ways that node $j$ might respond to its parents. In the case of binary nodes, this becomes $2^{\left(2^{|PA_j|}\right)}$. Thus for an endogenous node with no parents, there are 2 nodal types, for a binary node with one binary parent there are four types, for a binary node with 2 parents there are 16, and so on.

The set of all possible causal reactions of a given unit to all possible values of parents is then given by its collection of nodal types at each node. We call this collection a unit's "causal type", $\theta$.

The approach used by `CausalQueries` is to let the domain of $\theta^{Y_j}$ be coextensive with the number of nodal types for $Y_j$. Function $f^j$ then determines the value of $y$ by simply reporting the value of $Y_j$ implied by the nodal type and the values of the parents of $Y_j$. Thus if $\theta^j_{pa_j}$ is the value for $j$ when parents have values $pa_j$, then we have simply that $f_{Y_j}(\theta^{j}, pa_j) = \theta^j_{pa_j}$. The practical implication is that, given the causal structure, learning about the model reduces to learning about the distribution, $\lambda$, over the nodal types.

In cases in which there is no unobserved confounding, we take the probability distributions over the nodal types for different nodes to be independent: $\theta^i \perp\!\!\! \perp \theta^j, i\neq j$. In this case we use a categorical distribution to specify the ${\lambda^j_x} := \Pr(\theta^j = {\theta^j_x})$. From independence then we have that the probability of a given causal type $\theta_x$ is simply $\prod_{i=1}^n {\lambda^i_x}$. For instance $\Pr(\theta = (\theta^X_1, \theta^Y_{01})) = \Pr(\theta^X = \theta^X_1)\Pr(\theta^Y = \theta^Y_{01}) = \lambda^X_1\lambda^Y_{01}$.

In cases in which there is confounding, the logic is the same except that we need to specify enough parameters to capture the joint distribution over nodal types for different nodes. We do this by making use of the causal structure. 

As an example, for the Lipids model, the joint distribution of nodal types can be simplified as in @eq-join.

$$
\Pr(\theta^Z = \theta^Z_1, \theta^X = \theta^X_{10}, \theta^Y = \theta^Y_{11}) = 
\Pr(\theta^Z = \theta^Z_1)\Pr(\theta^X = \theta^X_{10})\Pr(\theta^Y = \theta^Y_{11}|\theta^X = \theta^X_{10})
$$ {#eq-join}

And so, for this model, $\lambda$ would include parameters that represent $\Pr(\theta^Z)$ and $\Pr(\theta^X)$ but also the conditional probability $\Pr(\theta^Y|\theta^X)$:

$$
\Pr(\theta^Z = \theta^Z_1, \theta^X = \theta^X_{10}, \theta^Y = \theta^Y_{11}) = 
\lambda^Z_1\lambda^X_{10}\lambda^{Y|\theta^X_{10}}_{11}
$$ {#eq-join2}

Representing beliefs *over causal models* thus requires specifying a probability distribution over $\lambda$. This might be a degenerate distribution if users want to specify a particular model. `CausalQueries` also allows users to specify parameters, $\alpha$ of a Dirichlet distribution over $\lambda$. If all entries of $\alpha$ are 0.5 this corresponds to Jeffreys priors. The default behavior is for `CausalQueries` to assume a uniform distribution -- that is, that all nodal types are equally likely -- which corresponds to $\alpha$ being a vector of 1s.

Updating is then done with respect to beliefs over $\lambda$. In the Bayesian approach we have simply:

$$p(\lambda|D) = \frac{p(D|\lambda)p(\lambda)}{\int_{\lambda^{'}} p(D|\lambda')p(\lambda')}$$

where $p(D|\lambda')$ is calculated under the assumption that units are exchangeable and independently drawn. In practice this means that the probability that two units have causal types $\theta_i$ and $\theta_j$ is simply $\lambda'_i\lambda'_j$. Since a causal type fully determines an outcome vector $d = \{y_1, y_2,\dots,y_n\}$, the probability of a given outcome ("event"), $w_d$, is given simply by the probability that the causal type is among those that yield outcome $d$. Thus, from $\lambda$ we can calculate a vector of event probabilities, $w(\lambda)$, for each vector of outcomes, and under independence, we have:

$$D \sim \text{Multinomial}(w(\lambda), N)$$

Thus for instance in the case of a $X \rightarrow Y$ model, and letting $w_{xy}$ denote the probability of a data type $X=x, Y=y$, the event probabilities are:

$$w(\lambda) = \left\{\begin{array}{ccc} w_{00} & = & \lambda^X_0(\lambda^Y_{00} + \lambda^Y_{01})\\ 
w_{01} & = & \lambda^X_0(\lambda^Y_{11} + \lambda^Y_{10})\\
w_{10} & = & \lambda^X_1(\lambda^Y_{00} + \lambda^Y_{10})\\
w_{11} & = & \lambda^X_1(\lambda^Y_{11} + \lambda^Y_{01})\end{array} \right.$$

For a more complex example @tbl-lipidspar illustrates key values for the Lipids model. We see here that we have two types for node $Z$, four for $X$ (representing the strata familiar from instrumental variables analysis: never takers, always takers, defiers, and compliers) and 4 for $Y$. For $Z$ and $X$ we have parameters corresponding to probability of these nodal types. For instance `Z.0` is the probability that $Z=0$. `Z.1` is the complementary probability that $Z=1$. Things are a little more complicated for distributions on nodal types for $Y$ however: because of confounding between $X$ and $Y$ we have parameters that capture the conditional probability of the nodal types for $Y$ *given* the nodal types for $X$. We see there are four sets of these parameters.

```{r}
#| label: tbl-lipidspar
#| tbl-cap: "Nodal types and parameters for Lipids model."
#| echo: false

with_pars <- 
  lipids_model |>
  set_parameters(param_type = "prior_draw") 

with_pars$parameters_df |>
  dplyr::select(node,  nodal_type, param_set, param_names, param_value, priors) |> 
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE,
    longtable = TRUE,
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("hold_position"))
```

The next to final column shows a sample set of parameter values. Together, the parameters describe a full joint probability distribution over types for $Z$, $X$ and $Y$ that is faithful to the graph.

From these we can calculate the probability of each data type. For instance the probability of data type $Z=0, X=0, Y=0$ is:

$$w_{000}=\Pr(Z=0, X=0, Y=0) = \lambda^Z_0\left(\lambda^X_{00}(\lambda^{Y|\lambda^X_{00}}_{00}+\lambda^{Y|\lambda^X_{00}}_{01}) + \lambda^X_{01}(\lambda^{Y|\lambda^X_{01}}_{00}+\lambda^{Y|\lambda^X_{01}}_{01})\right)$$

In practice `CausalQueries` generates a matrix, that maps from parameters into data types.

The value of the `CausalQueries` package is to allow users to specify *arbitrary* models of this form, figure out all the implied nodal types and causal types, and then update given priors and data by calculating event probabilities implied by all possible parameter vectors and in turn the likelihood of the data given the model. In addition, the package allows for arbitrary querying of a model to assess the values of estimands of interest that are a function of the values or counterfactual values of nodes, *conditional* on values or counterfactual values of nodes.

The following sections review key functionality for making, updating, and querying causal models.

## Making models {#sec-make}

A model is defined in one step in `CausalQueries` using a `dagitty` syntax [@textor_robust_2016] in which the model structure is provided as a statement. For instance:

```{r}
#| echo: true
#| results: markup

model <- make_model("X -> M -> Y <- X")
```

The statement in quotes, `"X -> M -> Y <- X"`, provides the names of nodes. An arrow ("`->`" or "`<-`") connecting nodes indicates that one node is a potential cause of another, i.e., whether a given node is a "parent" or "child" of another. Formally, a statement like this is interpreted as:

1.  Functional equations:

    -   $Y = f(M, X, \theta^Y)$
    -   $M = f(X, \theta^M)$
    -   $X = \theta^X$

2.  Distributions on $\Theta$:

    -   $\Pr(\theta^i = \theta^i_k) = \lambda^i_k$

3.  Independence assumptions:

    -   $\theta_i \perp\!\!\! \perp \theta_j, i\neq j$

Function $f$ maps from the set of possible values of the parents of $i$ to values of node $i$ given $\theta^i$ as described above.

In addition, as we did in the @chickering_clinicians_1996 example, it is possible to use two-headed arrows ("`<->`") to indicate "unobserved confounding," that is, the presence of an unobserved variable that might influence two or more observed variables. In this case, condition 3 above is relaxed, and the exogenous nodes associated with confounded variables have a joint distribution. We describe how this is done in greater detail in @sec-confounding.

### Graphing

Plotting the model can help check that you have defined the structure of the model correctly. `CausalQueries` provides simple graphing tools that draw on functionality from the `dagitty`, `ggplot2`, and `ggdag` packages.

Once defined, a model can be graphed by calling the `plot()` method on the objects with class `causal_model`. This method is a wrapper for the `plot_model()` function and accepts additional options described in `?plot_model`.

@fig-plots shows figures generated by plotting `lipids_model` with and without options. The plots have class `c("gg", "ggplot")` and so will accept any additional layers available for the objects of class `ggplot`.

```{r}
#| label: fig-plots
#| echo: true
#| fig-cap: "Examples of model graphs."
#| fig-subcap:
#|   - "Without options"
#|   - "With options"
#| fig-pos: 'h'
#| layout-ncol: 2
#| results: hold
#| purl: true

lipids_model |> plot()

lipids_model |>
  plot(x_coord = 1:3,
       y_coord = c(3,2,1),
       textcol = "white",
       textsize = 3,
       shape = 18,
       nodecol = "grey",
       nodesize = 12)

```

### Model characterization

When a model is defined, a set of objects is generated. These are the key quantities that are used for all inferential tasks. These objects can be accessed by using the `grab()` function. @tbl-core summarizes the core components of a model, providing a brief explanation for each one.

The first element is a `statement` which defines how the nodes in the model are related, specified by the user using `dagitty` syntax. The second element, `dag`, is a data frame that characterizes the parent-child relationships within the model. The element `nodes` is simply a list of the names of the nodes in the model. Fourth, the `parents_df,` is a table listing the nodes, indicating if they are "root" nodes (nodes with no parents among the set of specified nodes) and showing how many parents each node has.

The model includes additional elements: `parameters_df`, `nodal_types`, and `causal_types,` which we will explain later.

+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Element         | Description                                                                                                                                                         |
+=================+=====================================================================================================================================================================+
| `statement`     | A character string that describes directed causal relations between variables in a causal model, where arrows denote that one node is a potential cause of another. |
+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `dag`           | A data frame with columns 'parent' and 'children' indicating how nodes relate to each other.                                                                        |
+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `nodes`         | A list containing the nodes in the model.                                                                                                                           |
+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `parents_df`    | A table listing nodes, whether they are root nodes or not, and the number of parents they have.                                                                     |
+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `nodal_types`   | A list with the nodal types in the model. See @sec-nodal-types for more details.                                                                                    |
+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `parameters_df` | A data frame linking the model's parameters with the nodal types of the model, as well as the family to which they belong. See @sec-param-df for more details.      |
+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `causal_types`  | A data frame listing causal types and the nodal types that produce them. (See @sec-causal-types.)                 
+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Core Elements of a Causal Model. {#tbl-core tbl-colwidths="\[30,70\]"}

After updating a model, two additional components are attached to it:

-   A posterior distribution of the parameters in the model generated by Stan. This distribution reflects the updated parameter values.

-   A list of other optional objects, `stan_objects`. The `stan_objects` can include the `stanfit` object and distributions over nodal types and event probabilities.

@tbl-additional summarizes the objects attached to the model after updating.

+--------------------------+------------------------------------------------------------------------------------------+
| Element                  | Description                                                                              |
+==========================+==========================================================================================+
| `posterior_distribution` | The posterior distribution of the updated parameters generated by Stan.                  |
+--------------------------+------------------------------------------------------------------------------------------+
| `stan_objects`           | A list of additional objects (see next rows).                                            |
+--------------------------+------------------------------------------------------------------------------------------+
| `data`                   | The data used for updating the model, always included in `stan_objects.`                 |
+--------------------------+------------------------------------------------------------------------------------------+
| `type_distribution`      | The updated distribution of the nodal types, appended to `stan_objects` by default.      |
+--------------------------+------------------------------------------------------------------------------------------+
| `w`                      | A mapping from parameters to event probabilities, optionally appended to `stan_objects`. |
+--------------------------+------------------------------------------------------------------------------------------+
| `stan_fit`               | The `stanfit` object generated by Stan, optionally appended to `stan_objects`.           |
+--------------------------+------------------------------------------------------------------------------------------+

: Additional Elements. {#tbl-additional tbl-colwidths="\[30,70\]"}

#### Parameters data frame {#sec-param-df}

When a model is created, `CausalQueries` attaches a "parameters data frame," which keeps track of model parameters, and how they relate to causal types. This becomes especially important for more complex models with confounding that might involve more complicated mappings between parameters and nodal types. In the case with no confounding, the nodal types *are* the parameters; in cases with confounding, there are generally more parameters than nodal types. We already saw a segment of a parameters data frame for a model with confounding in @tbl-lipidspar. To access the full parameters data frame, we can call the `grab()` function as follows:

```{r}
#| label: params-df
#| echo: true
#| eval: true
#| purl: true

make_model("X -> Y") |> 
  grab("parameters_df") 

```

As in @tbl-lipidspar, each row in the parameters data frame corresponds to a single parameter. The `print()` method for the objects of the `parameters_df` class also includes a short description of each of the columns included in the data frame. More precisely, the columns of the parameters data frame are:

-   `param_names` gives the name of the parameter in shorthand. For instance the parameter $\lambda^X_0 = \Pr(\theta^X = \theta^X_0)$ has the name `X.0`.
-   `node` indicates the node associated with the parameter.
-   `gen` indicates the place in the partial causal ordering (generation) of the node associated with the parameter.
-   `param_set` indicates which parameters group together to form a simplex. The parameters in a set have parameter values that sum to 1. In this example $\lambda^X_0 + \lambda^X_1 = 1$.
-   `nodal_type` indicates the nodal types associated with the parameter.
-   `param_value` gives the (possibly default) parameter values (probabilities).
-   `priors` gives (possibly default) Dirichlet priors arguments for parameters in a set. Values of $1$ ($.5$) for all parameters in a set imply uniform (Jeffreys) priors over this set.

#### Nodal types {#sec-nodal-types}

As described above, two units have the same *nodal type* at node $Y$, $\theta^Y$, if their outcome at $Y$ responds similarly to parents of $Y$.

A binary node with $k$ binary parents has $2^{2^k}$ nodal types because there are $2^k$ possible values of the $k$ parents and so $2^{2^k}$ ways to respond to these possible parental values. 

The full set of nodal types is identified and stored in the model object when a model is created. The labels for these nodal types indicate how the unit responds to parents' values. For instance, consider the model with two parents $X \rightarrow Y \leftarrow M.$ In such a case, the nodal types of $Y$ will have subscripts with four digits, each representing one of the possible combinations of values that $Y$ can take, given the values of its parents $X$ and $M.$ These combinations include the value of $Y$ when:

-   $X = 0$ and $M = 0$,
-   $X = 0$ and $M = 1$,
-   $X = 1$ and $M = 0$,
-   $X = 1$ and $M = 1$.

As the number of parents increases, keeping track of what each digit represents becomes more difficult. For instance, if $Y$ had three parents, its nodal types would have subscripts of eight digits, each associated with the value that $Y$ would take for each combination of the three parents. The `interpret_type()` function provides a clear map to identify what each digit in the subscript represents and can be called by the user as follows:

```{r}
#| label: lookup-types2
#| echo: true

make_model("X -> Y <- M; W -> Y") |> 
  interpret_type(nodes = "Y")

```


#### Causal types {#sec-causal-types}

Causal types are collections of nodal types. Two units are of the same *causal type* if they have the same nodal type at every node. For example in a $X \rightarrow M \rightarrow Y$ model, $\theta = (\theta^X_0, \theta^M_{01}, \theta^Y_{10})$ is a type that has $X=0$, $M$ responds positively to $X$, and $Y$ responds positively to $M$.

The full set of causal types is identified and stored in the model object when a model is created. Users can access causal types using `grab()` function as follows:

```{r}
#| label: causal-types
#| echo: true
#| eval: true
#| purl: true

lipids_model |> 
  grab("causal_types")

```

In our running example, the Lipids model, there are $2\times 4\times 4 = 32$ causal types. A model with $n_j$ nodal types at node $j$ has $\prod_j n_j$ causal types suggesting that the set of causal types can be large.

Knowledge of a causal type tells us what values a unit would take on all nodes and whether or not there are interventions. For example for a model $X \rightarrow M \rightarrow Y$ a type $\theta = (\theta^X_0, \theta^M_{01}, \theta^Y_{10})$ would imply data $(X=0, M=0, Y=1)$ absent any intervention.^[The converse of this, of course, is the key to updating: observation of data $(X=0, M=0, Y=1)$ result in more weight placed on $\theta^X_0$, $\theta^M_{01}$, and $\theta^Y_{10})$.] The general approach used by `CausalQueries` for calculating outcomes from causal types is given in @sec-propagation.

#### Parameter matrix

The parameters data frame keeps track of parameter values and priors for parameters but does not provide a mapping between parameters and the probability of causal types. The parameter matrix---the "$P$ matrix"---can be added to the model to provide this mapping. The $P$ matrix has a row for each parameter and a column for each causal type. For instance:

```{r}
#| label: get-param-matrix
#| echo: true
#| eval: true
#| purl: true

make_model("X -> Y") |> 
  grab("parameter_matrix")

```

The probability of a causal type is given by the product of the parameter values for parameters whose row in the $P$ matrix contains a $1$. Later (e.g., in @sec-confounding), we will see examples where the $P$ matrix helps track parameters created when confounding is added to a model.

The parameter matrix is generated automatically as needed, but to speed up operation it might be useful to add it to the model manually. This can be done using the `set_parameter_matrix()` function as follows:

```{r}
#| echo: true
#| eval: false
#| purl: true

make_model("X -> Y") |> 
  set_parameter_matrix()

```

### Tailoring models

When a `dagitty` statement is provided to `make_data()`, a model is formed with a set of default assumptions: in particular, no restrictions are placed on nodal types and flat priors are assumed over all parameters. These features can be adjusted after a model is formed.

#### Setting restrictions {#restrictions}

Sometimes it is helpful to constrain the set of types. In `CausalQueries` this is done at the level of nodal types, with restrictions on causal types following restrictions on nodal types.

To illustrate, in analyses of data with imperfect compliance, as in our Lipids model example, it is common to impose a monotonicity assumption: that $X$ does not respond negatively to $Z$. This is one of the conditions needed to interpret instrumental variables estimates as (consistent) estimates of the complier average treatment effect. In `CausalQueries` we can impose this assumption as follows:

```{r}
#| echo: true
#| eval: true

model_restricted <- 
  lipids_model |> 
  set_restrictions("X[Z=1] < X[Z=0]")
```

In words: we restrict the model by removing types for which $X$ decreases in $Z$. If we wanted to retain only this nodal type rather than remove it, we could do so by passing `keep = FALSE` as an argument to the `set_restrictions()` function call. Users can use `grab(model, "parameter_matrix")` to view the resulting parameter matrix in which both the set of parameters and the set of causal types are restricted.

Restrictions in `CausalQueries` can be set in many other ways:

-   Using nodal type labels:

```{r}
#| label: set-restrictions1
#| echo: true
#| eval: false
#| purl: true

model <- 
  lipids_model |>
  set_restrictions(labels = list(X = "01", Y = c("00", "01", "11")), 
                   keep = TRUE)

```

-   Using wildcards in nodal type labels:

```{r}
#| label: set-restrictions2
#| echo: true
#| eval: false
#| purl: true

model <- lipids_model |>
  set_restrictions(labels = list(Y = "?0"))
```

-   In models with confounding, restrictions can be added to nodal types conditional on the values of other nodal types using a `given` argument:

```{r}
#| label: set-restrictions3
#| echo: true
#| eval: false
#| purl: true
model <- lipids_model |>
  set_restrictions(labels = list(Y = c('00', '11')), given = 'X.00')
```

Setting restrictions sometimes involves using causal syntax (see @sec-syntax for a guide to the syntax used by `CausalQueries`). The help file in `?set_restrictions` provides further details and examples of restrictions users can set.

#### Allowing confounding {#sec-confounding}

Unobserved confounding between two (or more) nodes arises when the nodal types for the nodes are not independent. For instance, in the $X \rightarrow Y$ graph, there are $2$ nodal types for $X$ and $4$ for $Y$. There are thus $8$ joint nodal types (or causal types), as shown in @tbl-joint.

+-----------------+----------------------------------+----------------------------------+----------------------+
|                 | $\theta^X_{0}$                   | $\theta^X_{1}$                   | $\sum$               |
+:===============:+:================================:+:================================:+:====================:+
| $\theta^Y_{00}$ | $\Pr(\theta^X_0, \theta^Y_{00})$ | $\Pr(\theta^X_1, \theta^Y_{00})$ | $\Pr(\theta^Y_{00})$ |
+-----------------+----------------------------------+----------------------------------+----------------------+
| $\theta^Y_{10}$ | $\Pr(\theta^X_0, \theta^Y_{10})$ | $\Pr(\theta^X_1, \theta^Y_{10})$ | $\Pr(\theta^Y_{10})$ |
+-----------------+----------------------------------+----------------------------------+----------------------+
| $\theta^Y_{01}$ | $\Pr(\theta^X_0, \theta^Y_{01})$ | $\Pr(\theta^X_1, \theta^Y_{01})$ | $\Pr(\theta^Y_{01})$ |
+-----------------+----------------------------------+----------------------------------+----------------------+
| $\theta^Y_{11}$ | $\Pr(\theta^X_0, \theta^Y_{11})$ | $\Pr(\theta^X_1, \theta^Y_{11})$ | $\Pr(\theta^Y_{11})$ |
+-----------------+----------------------------------+----------------------------------+----------------------+
| $\sum$          | $\Pr(\theta^X_0)$                | $\Pr(\theta^X_1)$                | 1                    |
+-----------------+----------------------------------+----------------------------------+----------------------+

: Nodal types in $X \rightarrow Y$ model. {#tbl-joint tbl-colwidths="\[25,25,25,25\]"}

@tbl-joint has eight interior elements so that an unconstrained joint distribution would have $7$ degrees of freedom. A no-confounding assumption means that $\Pr(\theta^X | \theta^Y) = \Pr(\theta^X)$, or $\Pr(\theta^X, \theta^Y) = \Pr(\theta^X)\Pr(\theta^Y)$. In this case, we just put a distribution on the marginals, and there would be $3$ degrees of freedom for $Y$ and $1$ for $X$, totaling $4$ rather than $7$.

The parameters data frame for this model would have two parameter families for parameters associated with the node $Y$. Each family captures the conditional distribution of $Y$'s nodal types, given $X$. For instance the parameter `Y01_X.1` can be interpreted as $\Pr(\theta^Y = \theta^Y_{01} | \theta^X=1)$. See again @tbl-lipidspar for an example of a parameters matrix with confounding.

The confounding structure can affect the number of parameters given the underlying DAG. @tbl-dof illustrates the number of independent parameters required given different types of confounding.


```{r}
#| label: tbl-dof
#| tbl-cap: "Number of different independent parameters (degrees of freedom) for different three-node models."
#| echo: false
#| eval: true

statements <- list("X -> Y <- W", 
                   "X -> Y <- W; X <-> W", 
                   "X -> Y <- W; X <-> Y; W <-> Y",
                   "X -> Y <- W; X <-> Y; W <-> Y; X <-> W", 
                   "X -> W -> Y <- X",
                   "X -> W -> Y <- X; W <-> Y",
                   "X -> W -> Y <- X; X <-> W; W <-> Y", 
                   "X -> W -> Y <- X; X <-> W; W <-> Y; X <-> Y")

dof <- function(statement) {
  make_model(statement, add_causal_types = FALSE) |>
  grab("parameters_df")  |>
  group_by(param_set) |>
  summarize(n  = n() - 1) |>
  pull(n) |>
  sum()
}
  

statements |> 
  lapply(function(s) paste0("`", s, "`")) |> 
  unlist() |> 
  data.frame(
    Model = _,
    dof = unlist(lapply(statements, dof))) |>
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = c("l", "c"),
    escape = TRUE, 
    linesep = "",
    col.names = c("Model", "Degrees of freedom")) 

```

#### Setting Priors {#priors}

Priors on model parameters can be added to the parameters data frame and interpreted as alpha parameters of a Dirichlet distribution. The Dirichlet distribution is a probability distribution over an $n-1$ dimensional unit simplex. It can be considered a generalization of the Beta distribution and is parametrized by an $n$-dimensional positive vector $\alpha$. Thus, for example a Dirichlet with $\alpha = (1, 1, 1, 1, 1)$ gives a probability distribution over all non-negative $5$-dimensional vectors that sum to $1$, e.g. $(0.1, 0.1, 0.1, 0.1, 0.6)$ or $(0.1, 0.2, 0.3, 0.3, 0.1)$. This particular value for $\alpha$ implies that all such vectors are equally likely. Other values for $\alpha$ can be used to control the expectation and certainty for each dimension. For instance, the vector $\alpha = (100, 1, 1, 1, 100)$ would result in more weight on distributions that are close to $(0.5, 0, 0, 0, 0.5)$.

In `CausalQueries`, priors are generally specified over the distribution of nodal types.^[If there is confounding in the model, priors are specified over the conditional distribution of nodal types.] For instance, in a model represented by $X \rightarrow Y$, we have one Dirichlet distribution over the two types for $\theta^X$ and one Dirichlet distribution over the four types for $\theta^Y$.

Importantly, it is implicitly assumed that priors are independent across families. Thus, for instance, in a model represented by $X \rightarrow Y$, we specify beliefs over $\lambda^X$ and over $\lambda^Y$ separately. `CausalQueries` does not let users specify correlated beliefs over these parameters.[^4]

[^4]: Users can specify beliefs about $\lambda^Y$ given $\theta^X$ if a model involves possible confounding. But this statement is about beliefs over a joint distribution, not jointly distributed beliefs.

Prior hyperparameters are set to unity by default, corresponding to uniform priors. Users can retrieve the model's priors as follows:

```{r}
#| label: get-priors
#| echo: true
#| eval: true

lipids_model |> 
  grab("prior_hyperparameters", "X") 

```

Alternatively users can set Jeffreys priors using `set_priors()` as follows:

```{r}
#| label: set-priors
#| echo: true
#| eval: false
#| purl: true

model <- lipids_model |> 
  set_priors(distribution = "jeffreys")

```

Users can also provide custom priors. The simplest way to specify custom priors is to add them as a vector of numbers using `set_priors()`. For instance:

```{r}
#| label: set-priors-custom
#| echo: true
#| eval: true
#| message: FALSE

lipids_model |> 
  set_priors(node = "X", alphas = 1:4) |> 
  grab("prior_hyperparameters", "X")

```

The priors here should be interpreted as indicating $\alpha_X = (1,2, 3, 4)$, which implies a distribution over $(\lambda^X_{00},\lambda^X_{10}, \lambda^X_{01}, \lambda^X_{11})$ centered on $\left(\frac1{10}, \frac2{10}, \frac3{10}, \frac4{10} \right)$.

Providing priors as a vector of numbers for larger models can be hard. For that reason, `set_priors()` allows for more targeted modifications of the parameter vector. For instance:

```{r}
#| label: set-priors-statement
#| echo: true
#| eval: true

lipids_model |>
  set_priors(statement = "X[Z=1] > X[Z=0]", alphas = 3) |>
  grab("prior_hyperparameters", "X")

```

Setting priors requires mapping alpha values to parameters, and so the problem of altering priors reduces to selecting rows of the `parameters_df` data frame at which to alter values. When specifying a causal statement as above, `CausalQueries` internally identifies nodal types consistent with the statement, which identifies parameters to alter priors for.

We can achieve the same result as above by specifying nodal types for which we would like to adjust the priors. 
`set_priors()` allows for the specification of any non-redundant combination of arguments on the `param_names`, `node`, `nodal_type`, `param_set`, and `given` columns of `parameters_df` to identify parameters to set priors for uniquely. Alternatively, a fully formed subsetting statement may be supplied to `alter_at`. Since all these arguments are mapped to the parameters they identify internally, they may be used interchangeably.[^5]

[^5]: See `?set_priors` and `?make_priors` for many more examples.

While highly targeted prior setting is convenient and flexible, it should be used cautiously. Setting priors on specific parameters in complex models, especially models involving confounding, may strongly affect inferences. Furthermore, note that flat priors over nodal types do not necessarily translate into flat priors over queries. Flat priors over parameters in a parameter family put equal weight on each nodal type, which can translate into strong assumptions on causal quantities of interest. For instance, in an $X \rightarrow Y$ model in which negative effects are ruled out, the average causal effect implied by flat priors is $1/3$. This can be seen by querying the model as follows:

```{r}
#| label: set-priors-flat
#| echo: true
#| eval: false
#| purl: false

make_model("X -> Y") |>
  set_restrictions(decreasing("X", "Y")) |>
  query_model("Y[X=1] - Y[X=0]", using = "priors")

```

More subtly, the *structure* of a model, coupled with flat priors, has substantive importance for priors on causal quantities. For instance, with flat priors, prior on the probability that $X$ has a positive effect on $Y$ in the model $X \rightarrow Y$ is centered on $1/4$. But prior on the probability that $X$ positively affects $Y$ in the model $X \rightarrow M \rightarrow Y$ is centered on $1/8$.

Caution regarding priors is essential when models are not identified, as is the case for many models considered here. For some quantities, the marginal posterior distribution reflects the marginal prior distribution [@poirier_revising_1998].

The crucial aspect we emphasize is the necessity of avoiding the misconception that "uninformative" priors are devoid of implications concerning the values of causal quantities of interest. In reality, these priors do carry certain presumptions. The impact of flat priors on causal quantities is contingent on the structural configuration of the model. Moreover, for some inferences from causal models, the priors can matter a lot even if you have a lot of data. In such cases, it can be helpful to know what priors on parameters imply for priors on causal quantities of interest (by using `query_model()`) and to assess how much conclusions depend on priors (by comparing results across models that vary in their priors).

The following code gives an example where a change in model structure and uniform priors imply different beliefs over causal quantities.

```{r}
#| label: compare-flat-priors
#| echo: true
#| eval: false
#| purl: true
make_model("X -> Y") |>
  query_model("Y[X=1] > Y[X=0]", using = "priors")

make_model("X -> M -> Y") |>
  query_model("Y[X=1] > Y[X=0]", using = "priors")
```

#### Setting Parameters {#parameters}

By default, models have a vector of parameter values in the `parameters_df` data frame. These are useful for generating data or for situations, such as process tracing, when one wants to make inferences about causal types ($\theta$), given case-level data, under the assumption that the model is known.

The logic for setting parameters is similar to setting priors: we need to place values on the probability of nodal types. The critical difference is that whereas the $\alpha$ value placed on nodal types can be any positive number---capturing our certainty over the parameter value---the parameter values must lie in the unit interval, $[0,1]$. In general, if passed parameter values do not lie in the unit interval, they are normalized so that they do.

Consider the causal model below. It has two parameter sets, one for $X$ and one for $Y$, with six nodal types, two corresponding to $X$ and four corresponding to $Y$. The key feature of the parameters is that they must sum to $1$ within each parameter set.

```{r}
#| label: get-parameters
#| echo: true
#| eval: true

make_model("X -> Y") |> 
  grab("parameters")
```

The example below illustrates a change in the value of the parameter $Y$ if it increases in $X$. Here, the nodal type `Y.Y01` is set to be $0.7$, while the other nodal types of this parameter set were re-normalized so that the parameters in the set still sum up to one.

```{r}
#| label: set-parameters
#| echo: true
#| eval: true

make_model("X -> Y") |>
  set_parameters(statement = "Y[X=1] > Y[X=0]", parameters = .7) |>
  grab("parameters")

```

### Drawing and manipulating data

Once a model has been defined, it is possible to simulate data from the model using the `make_data()` function. For instance, this can be useful for assessing a model's expected performance given data drawn from some speculated set of parameter values.

#### Drawing data basics

Generating data requires a specification of parameter values that users can provide. If not provided, default values place equal weight on all nodal types.

```{r}
#| label: make-data
#| echo: true
#| eval: true

sample_data_1 <- 
  lipids_model |> 
  make_data(n = 4)

```

However, users can also specify parameters directly or draw parameters from a prior or posterior distribution. For instance:^[Note that the returned data is ordered by data type, as shown in the example above.]

```{r}
#| label: make-data-draw
#| echo: true
#| eval: true

lipids_model |>
  make_data(n = 3, param_type = "prior_draw")

```

#### Drawing incomplete data

`CausalQueries` can be used when researchers have gathered different amounts of data for different nodes. For instance, the researchers could collect data on $X$ and $Y$ for all units, but data on $M$ would only be for some. The function `make_data()` allows you to draw data like this if you specify a data strategy indicating the probabilities of observing data on different nodes, possibly as a function of prior nodes observed.

```{r}
#| label: make-data-incomplete
#| echo: true
#| eval: true
#| message: false

sample_data_2 <-
  lipids_model |>
  make_data(n = 8,
            nodes = list(c("Z", "Y"), "X"),
            probs = list(1, .5),
            subsets = list(TRUE, "Z==1 & Y==0"),
            verbose = FALSE)

sample_data_2
```

#### Reshaping data

Whereas data usually comes in long form, with a row per observation, the data passed to Stan during model updating is in a "compact" form. The latter records only the number of units of each data type, grouped by data "strategy"---an indicator of the nodes for which researchers gathered data. `CausalQueries` includes functions that let users move between these two forms.

```{r}
#| label: collapse-data
#| echo: true
#| eval: true

sample_data_2 |> 
  collapse_data(lipids_model)
```

In the same way, it is possible to move from compact to long data using `expand_data()`. Note that `NA`'s are interpreted as data not being sought. So in the case of `sample_data_2`, the interpretation is that there are two data strategies: data on $Y$, $M$, and $X$ was sought in two cases only while in six cases only data on $Y$ and $X$ was sought.

## Updating models {#sec-update}

The approach used by the `CausalQueries` package to update parameter values given observed data relies on the Stan programming language [@carpenter_stan_2017]. Below we explain the data required by the generic Stan program implemented in the package, the structure of that program, and then show how to use the package to produce posterior draws of parameters.

### Data for Stan

We use a generic Stan program that works for all binary causal models. The main advantage of the generic program we implement is that it allows us to pass the details of the causal model as data inputs to Stan instead of generating individual Stan programs for each causal model. [Appendix B](#sec-stancode) provides the complete Stan model code.

The data required by the Stan program includes vectors of observed data and priors on parameters, as well as a set of matrices needed for the mapping between events, data types, causal types, and parameters. In addition, data includes counts of all relevant quantities as well as start and end positions of parameters pertaining to specific nodes and distinct data strategies.

The internal function `prep_stan_data()` takes the model and data as arguments and produces a list with all objects that are required by the generic Stan program. Package users do not need to call the `prep_stan_data()` function directly.

### How the Stan program works

The Stan model involves the following elements: (1) a specification of priors over sets of parameters, (2) a mapping from parameters to event probabilities,  and (3) a likelihood function. Below, we describe each of those elements in more detail.

#### Probability distributions over parameter sets

The causal structure provided by a DAG allows us to reduce the problem of generating a probability distribution over all parameters to one of generating distributions over  "sets" of parameters. Without unobserved confounding, these sets correspond to the nodal types for each node: we have a probability distribution over the set of nodal types. In cases with confounding, these are sets of nodal types for a given node *given* values of other nodes: we have to characterize the probability of each nodal type in a set given the values of nodal types for other nodes.

To illustrate, we have two parameter sets in the $X \rightarrow Y$ model. The first is $\lambda^X \in \{\lambda^X_0, \lambda^X_1\}$ whose elements give the probability that $X$ is $0$ or $1$. These two probabilities sum up to one. The second parameter set is $\lambda^Y \in \{\lambda^Y_{00}, \lambda^Y_{10}, \lambda^Y_{01} \lambda^Y_{11}\}$. These are also probabilities, and their values sum up to one. Note that we have $6$ parameters, but just $1 + 3 = 4$ degrees of freedom.

We express priors over these parameter sets using multiple Dirichlet distributions. Thus for instance we have $(\lambda^X_0, \lambda^X_1) \sim Dirichlet(\alpha^X_0, \alpha^X_1).$ Overall, in the $X \rightarrow Y$ model, we have a $2$-dimensional Dirichlet distribution over the $X$ nodal types and a $4$-dimensional Dirichlet over the $Y$ nodal types.

#### Event probabilities

We calculate the probability of data types for any candidate parameter vector $\lambda$. This is done using a matrix that maps from parameters into data types. 

In cases without confounding, there is a column for each data type; the matrix indicates which nodes in each set "contribute" to the data type, and the probability of the data type is found by summing within sets and taking the product over sets. To illustrate, we can examine the parameter mapping matrix for a simple model using the `grab()` function as follows:

```{r}
#| echo: true
#| eval: true
#| purl: true 

make_model("X -> Y") |> 
  grab("parameter_mapping") 

```

In this model, the probability of data type `X0Y0`, $w_{00}$ is $\lambda^X_0\times \lambda^Y_{00} + \lambda^X_0\times \lambda^Y_{01}$. This formula can be read from the parameter mapping matrix by combining a parameter vector with the first column of the matrix, taking the product of the probability of `X.0` and the *sum* of the probabilities for `Y.00` and `Y.01`.

In cases with confounding, the approach is similar, except that the parameter mapping matrix can contain multiple columns for each data type to capture non-independence between nodes.

In the case of incomplete data, we first identify the set of data strategies, where a collection of a data strategy might be of the form "gather data on $X$ and $M$, but not $Y$, for $n_1$ cases and gather data on $X$ and $Y$, but not $M$, for $n_2$ cases." Within a data strategy, the probability of an observed event is given by summing the probabilities of the types that could give rise to incomplete data. 

#### Data probability

Once we have the event probabilities in hand for each data strategy, we are ready to calculate the probability of the data. For a given data strategy, this is given by a multinomial distribution with these event probabilities. When there is incomplete data, and so there are multiple data strategies, the product of the multinomial probabilities for each strategy gives this.

### Implementation

The function `update_model()` can be used to update model as follows:

```{r}
#| label: update-model
#| echo: true
#| eval: false
#| purl: true
model <- update_model(model, data)
```

The `data` argument provides a data frame containing some or all of the nodes in the model. The function `update_model()` relies on `rstan::sampling()` to draw from the posterior distribution, and one can pass any additional arguments accepted by `rstan::sampling()` in `...`. Given that the model updating can sometimes be slow for complex models in [Appendix A](#sec-parallel), we show how users can utilize parallelization to improve computation speed. [Appendix C](#sec-benchmark) provides an overview of model updating benchmarks, evaluating the effects of model complexity and data size on updating times.

### Incomplete and censored data

`CausalQueries` assumes that missing data is missing at random, conditional on observed data. For instance, in an $X \rightarrow M \rightarrow Y$ model, a researcher might have chosen to collect data on $M$ in a random set of cases in which $X=1$ and $Y=1$. If there are positive relations at each stage, you may be more likely to observe $M$ in cases in which $M=1$. However, the observation of $M$ is still random and conditional on the observed $X$ and $Y$ data. The Stan model in `CausalQueries` takes account of this kind of sampling by assessing the probability of observing a particular pattern of data within each data strategy.^[For further discussion, see Section 9.2.3.2 in @humphreys_integrated_2023.]

In addition, it is possible to indicate when data has been censored and for the Stan model to take this into account also. For instance, consider a situation where we only observe $X$ in cases where $X=1$ and not where $X=0$. This kind of sampling is non-random and conditional on observables. It can be taken into account, however, by indicating to Stan that the probability of observing a particular data type is $0$, regardless of parameter values. This can done using the `censored_types` argument in `update_model()`.

To illustrate, in the example below, we observe perfectly correlated data for $X$ and $Y$. If we are aware that data in which $X \neq Y$ has been censored, then when we update, we do not move towards a belief that $X$ causes $Y$.

```{r}
#| echo: true
#| eval: true
#| purl: true

data <- data.frame(X = rep(0:1, 5), Y = rep(0:1, 5))

list(
  uncensored = 
    update_model(make_model("X -> Y"),
                 data),
  censored = 
    update_model(make_model("X -> Y"), 
                 data, 
                 censored_types = c("X1Y0",  "X0Y1"))
  ) |>
  query_model("Y[X=1] - Y[X=0]", using = "posteriors") |> 
  subset(select = c("model", "query", "mean", "sd"))

```

### Output

The primary output from `update_model()` is a model with an attached posterior distribution over model parameters stored as a data frame in the model list. This posterior distribution can be directly accessed using the `grab()` function as follows:

```{r}
#| eval: false
#| echo: true
#| purl: true

make_model("X -> Y")  |> 
  update_model() |>
  grab("posterior_distribution")  

```

In addition, a distribution of causal types is stored by default; the `stanfit` object and a distribution over event probabilities are optionally saved as follows: 

```{r}
#| eval: false
#| echo: true
#| purl: true

lipids_model <- 
  lipids_model |> 
  update_model(keep_fit = TRUE,
               keep_event_probabilities = TRUE)

```

The summary of the Stan model can be accessed using `grab()` function and is saved in updated model object by default. This provides two measures to help assess convergence.

```{r}
#| eval: true
#| echo: true
#| purl: true

make_model("X -> Y")  |> 
  update_model(keep_type_distribution = FALSE) |>
  grab("stan_summary") 

```


This summary provides information on the distribution of parameters and convergence diagnostics, summarized in the `Rhat` column. The last row shows the unnormalized log density on Stan's unconstrained space, which is intended to diagnose sampling efficiency and evaluate approximations.^[See [Stan documentation](https://mc-stan.org/cmdstanr/reference/fit-method-lp.html) for more details.] This summary can also include summaries for the transformed parameters if users retain these.^[See @tbl-additional for options.]

If users wish to run more advanced diagnostics of performance, they can retain and access the "raw" Stan output as follows: 

```{r}
#| eval: true
#| echo: true
#| purl: true

model <- 
  make_model("X -> Y") |> 
  update_model(refresh = 0, keep_fit = TRUE)

```

Note that the raw output uses labels from the generic Stan model: `lambda` for the vector of parameters, corresponding to the parameters in the parameters data frame (`grab(model, "parameters_df")`), a vector `types` for the causal types (`grab(model, "causal_types")`) and `event_probabilities` for the event probabilities (`grab(model, "event_probabilities")`). 

```{r}
#| eval: true
#| echo: true
#| purl: true

model |> 
  grab("stan_fit")

```

Users can pass the Stan fit object to other diagnostic packages such as `bayesplot`.

<!-- ```{r}
np <- model |> grab("stan_fit") |> bayesplot::nuts_params()

model |> grab("stan_fit") |>
  bayesplot::mcmc_trace(pars = "lambdas[5]", np = np) +
  ylab("trace for fifth element of lambda")
``` -->

## Queries {#sec-query}

`CausalQueries` provides functionality to pose and answer elaborate causal queries. The key approach is to code causal queries as functions of causal types and return a distribution over the queries implied by the distribution over causal types.

### Calculating factual and counterfactual quantities {#sec-propagation}

An essential step in calculating most queries is assessing what outcomes will arise for causal types given different interventions on nodes. In practice, we map from causal types to data types by propagating realized values on nodes forward in the DAG, moving from exogenous or intervened upon nodes to their descendants in generational order. The `realise_outcomes()` function achieves this by traversing the DAG while recording the values implied by realizations on the node's parents for each node's nodal types.

To illustrate, consider the first causal type of a $X \rightarrow Y$ model:

1.  $\theta^X_0$ implies that, absent intervention on $X$, $X$ has a realized value of $0$; $\theta^Y_{00}$ implies that, absent intervention on $Y$, $Y$ has a realized value of $0$ regardless of $X$
2.  We substitute for $Y$ the value implied by the $00$ nodal type given a $0$ value on $X$, which in turn is $0$ (see @sec-nodal-types).

Calling `realise_outcomes()` on this model yields the outcomes implied by all causal types:

```{r}
#| label: realise-outcomes
#| echo: true

make_model("X -> Y") |> 
  realise_outcomes()

```

The output above shows realized values with row names indicating corresponding nodal types. Intervening on $X$ [see @pearl_causality_2009] with $do(X=1)$ yields:

```{r}
#| label: realise-outcomes-do
#| echo: true

make_model("X -> Y") |> 
  realise_outcomes(dos = list(X = 1))

```

In the same way, `realise_outcomes()` can return the realized values on all nodes for each causal type given arbitrary interventions.

### Causal syntax {#sec-syntax}

`CausalQueries` provides syntax for the formulation of various causal queries including queries on all rungs of the "causal ladder" [@pearl_causality_2009]: prediction, such as the proportion of units where $Y$ equals $1$; intervention, such as the probability that $Y = 1$ when $X$ is *set* to $1$; counterfactuals, such as the probability that $Y$ would be $1$ were $X = 1$ given we know $Y$ is $0$ when $X$ was observed to be $0$. Queries can be posed at the population level or case level and can be unconditional (e.g., what is the effect of $X$ on $Y$ for all units) or conditional (for example, the effect of $X$ on $Y$ for units for whom $Z$ affects $X$). This syntax enables users to write arbitrary causal queries to interrogate their models.

The heart of querying is figuring out which causal types correspond to particular queries. Users may employ logical statements to ask questions about observed conditions without intervention for factual queries. Take, for example, the query mentioned above about the proportion of units where $Y$ equals $1$, expressed as `"Y == 1"`. In this case, the logical operator `==` indicates that `CausalQueries` should consider units that fulfill the condition of strict equality where $Y$ equals $1$.[^6] When this query is posed, the `get_query_types()` function identifies all types that give rise to $Y=1$, absent any interventions.

[^6]: `CausalQueries` also accepts = as a shorthand for ==. However, == is preferred as it is the conventional logical operator to express a condition of strict equality.

```{r}

make_model("X -> Y")  |> 
  get_query_types("Y==1")

```

The key to forming causal queries is being able to ask about the values of variables, given that the values of some other variables are "controlled." This corresponds to the application of the $do$ operator in @pearl_causality_2009. In `CausalQueries`, this is done by putting square brackets `[ ]` around variables that should be intervened upon.

For instance, consider the query `Y[X=0]==1`. This query asks about the types for which $Y$ equals $1$ when $X$ is set to $0$. Since $X$ is being intervened to be zero, $X$ is placed inside the brackets. Given that $Y$ equaling $1$ is a condition about potentially observed values, it is expressed using the logical operator `==`. The set of causal types that meets this query is quite different:

```{r}

make_model("X -> Y")  |> 
  get_query_types("Y[X=1]==1")

```

When a node has multiple parents, it is possible to set the values of none, some, or all of the parents. For instance if $X1$ and $X2$ are parents of $Y$ then `Y==1`, `Y[X1=1]==1`, and `Y[X1=1, X2=1]==1` queries cases for which $Y=1$ when, respectively, neither parents values are controlled, when $X1$ is set to $1$ but $X2$ is not controlled, and when both $X1$ and $X2$ are set to $1$. For instance:

```{r}
#| eval: true
#| echo: true

make_model("X1 -> Y <- X2")  |>
  get_query_types("X1==1 & X2==1 & (Y[X1=1, X2=1] > Y[X1=0, X2=0])")

```

In this case, the aim is to identify the types for which $X1=X2=1$ *and at the same time* $Y=0$ when $X1 = X2 = 0$, and $Y = 1$ when $X1 = X2 = 1$.

In general, as if conducting an experiment, the variables to be intervened upon are placed inside square brackets, followed by an equal sign and the value to which we want to set them, either $1$ or $0$. The variable whose value is observed should be placed before the square brackets. Thus `"Y[X=1]"` queries the values of $Y$ when $X$ is set to $1$. Finally, conditions related to observed or potentially observed values, in the context of an intervention, are expressed outside the brackets, along with the logical condition that defines the observed values, as in `"Y==1"`, `"Y[X=1]==1` or `"Y[X=1] > Y[X=0]"`.

#### Conditional queries

Many queries of interest are "conditional" queries. For example, the effect of $X$ on $Y$ for units for which $W= 1$ or the effect of $X$ on $Y$ for units for which $Z$ positively affects $X$. Such conditional queries are posed in `CausalQueries` by providing a `given` statement and the `query` statement. The entire query then becomes: for what units does the `query` condition hold among those units for which the `given` condition holds? The two parts can each be calculated using `get_query_types`. Thus, for instance, in an $X \rightarrow Y$ model, the probability that $X$ causes $Y$ given $X=1 \, \& \, Y=1$ is the probability of causal `X1.Y11` type divided by the sum of the probabilities of types `X1.Y11` and `X1.Y01`. In practice, this is done automatically for users when they call `query_model()` or `query_distribution()`.

```{r}
#| eval: false
#| echo: false
#| purl: false
model <- make_model('X -> Y') 

query <- model |> get_query_types(query = "Y[X=1] > Y[X=0]")
given <- model |> get_query_types(query = "Y==1 & X==1")

model$causal_types |> 
  mutate(query = query$types,
         given = given$types)

```

#### Complex expressions

Many queries involve complex statements over multiple sets of types. These can be formed with the aid of relational operators. For example, you can make queries about cases where $X$ has a positive effect on $Y$, i.e., whether $Y$ is greater when $X$ is set to $1$ compared to when $X$ is set to $0$, expressed as `"Y[X=1] > Y[X=0]"`. The query "$X$ has some effect on $Y$" is given by `"Y[X=1] != Y[X=0]"`.

Linear operators can also be used over a set of simple statements. Thus `"Y[X=1] - Y[X=0]"` returns the average treatment effect. In essence, rather than returning a `TRUE` or `FALSE` for the two parts of the query, the case memberships are forced to numeric values ($1$ or $0$), and the differences are taken, which can be a $1$, $0$ or $-1$ depending on the causal type. Averaging provides the share of cases with positive effects, less the share of cases with negative effects.

```{r}
make_model("X -> Y") |> 
  get_query_types("Y[X=1] - Y[X=0]")

```

#### Nested queries

`CausalQueries` lets users pose nested "complex counterfactual" queries. For instance `"Y[M=M[X=0], X=1]==1"` queries the types for which $Y$ equals $1$ when $X$ is set to $1$, while keeping $M$ constant at the value it would take if $X$ were $0$.

### Quantifying queries

Giving a *quantitative* answer to a query requires placing probabilities over the causal types that correspond to a query.

#### Queries by hand

Queries can be calculated directly from the prior distribution or the posterior distribution provided by Stan. For instance, the following call plots the posterior distribution for the query that the probability of $Y$ is increasing in $X$ for the $X \rightarrow Y$ model. The resulting plot is shown in @fig-posterior-dist.

```{r}
#| label: fig-posterior-dist
#| fig-cap: 'Posterior on "Probability $Y$ is increasing in $X$".'
#| fig-pos: "t"
#| fig-align: center
#| out-width: "60%"
#| purl: true

data  <- data.frame(X = rep(0:1, 50), Y = rep(0:1, 50))

model <- 
  make_model("X -> Y") |>
  update_model(data, iter  = 4000, refresh = 0)

model |> 
  grab("posterior_distribution")  |> 
  ggplot(aes(Y.01 - Y.10)) + geom_histogram() 

```


\FloatBarrier

#### Query distribution

It is generally helpful to use causal syntax to define the query and calculate the query with respect to the prior or posterior probability distributions. This can be done for a list of queries using `query_distribution()` function as follows:

```{r}
#| eval: false
#| echo: true
#| purl: false

make_model("X -> Y") |> 
  query_distribution(
    query = list(increasing = "(Y[X=1] > Y[X=0])"), 
    using = "priors")

```

`query_distribution()` can also be used when one is interested in assessing the value of a query for a *particular case*. In a sense, this is equivalent to posing a conditional query, querying conditional on values in a case. For instance, we might consult our posterior for the Lipids model and ask about the effect of $X$ on $Y$ for a case in which $Z=1$, $X=1$ and $Y=1$.

```{r}
#| label: case-level-query
#| echo: true
#| eval: true
#| purl: true

lipids_model |>
  query_model(query = "Y[X=1] - Y[X=0]",
              given = "X==1 & Y==1 & Z==1",
              using = "posteriors")  |> 
  subset(select = c("query", "mean", "sd"))

```

The result is what we now believe for all cases in which $Z=1$, $X=1$, and $Y=1$. It is the expected average effect among cases with this data type, so this expectation has an uncertainty attached.

This is, in principle, different from what we would infer for a "new case" that we wonder about. When inquiring about a new case, the case-level query *updates* on the given information observed in the new case. The resulting inference can be different from the inference that would be made from the posterior *given* the case features. This new case-level inference is calculated if the `case_level = TRUE` argument is specified. For a query $Q$ and given $D$ this returns the value $\frac{\int\pi(Q \& D | \lambda_i)p(\lambda_i)d\lambda_i}{\int\pi(D | \lambda_i)p(\lambda_i)d\lambda_i}$ which may differ from the mean of the distribution $\frac{\pi(Q \& D | \lambda)}{\pi(D | \lambda)}$, $\int \frac{\pi(Q \& D | \lambda_i)}{\pi(D | \lambda_i)} p(\lambda_i)d\lambda_i$.

To simplify, consider a model where it is clear that $X$ causes $Y$, but it is uncertain if this is through two positive or two negative effects. If we encounter a case with $M=0$, it is unclear whether this indicates an effect. However, if we randomly find a case with $M=0$, our understanding of the causal model evolves, leading us to believe there is an effect in this specific case, which would not be the case if $M=1$. The case-level query gives a single value without posterior standard deviation, representing the belief about this new case. The non case-level query summarizes the posterior distribution for cases with similar data.


```{r}
#| echo: true
#| eval: true
#| purl: true

make_model("X -> M -> Y") |>
  update_model(data.frame(X = rep(0:1, 8), Y = rep(0:1, 8)), iter = 4000) |>
  query_model("Y[X=1] > Y[X=0]", 
            given = "X==1 & Y==1 & M==1", 
            using = "posteriors",
            case_level = c(TRUE, FALSE))  |> 
  subset(select = c("query", "case_level", "mean", "sd"))

```

#### Batch queries

The function `query_model()` can also be used to query multiple models. The function takes a list of models, causal queries, and conditions as input. It then calculates population or case level estimands given prior or posterior distributions and reports summaries of these distributions. The result is a data frame that can be displayed as a table or used for graphing. 

@tbl-batch-query returns to the Lipids data and shows the output from a single call to `query_model()` with the `expand_grid` argument set to `TRUE` to generate all combinations of list elements.

```{r}
#| label: batch-query
#| echo: true
#| eval: true
#| purl: false

models <- list(
  A = lipids_model |> 
    update_model(data = lipids_data, refresh = 0),
  B = lipids_model |> set_restrictions("X[Z=1] < X[Z=0]") |>
    update_model(data = lipids_data, refresh = 0))

queries <- 
  query_model(
  models,
  query = list(ATE = "Y[X=1] - Y[X=0]", 
               POS = "Y[X=1] > Y[X=0]"),
  given = c(TRUE,  "Y==1 & X==1"),
  case_level = c(FALSE, TRUE),
  using = c("priors", "posteriors"),
  expand_grid = TRUE)

```

```{r}
#| label: tbl-batch-query
#| tbl-cap: "Results for two queries on two models."
#| echo: false
#| eval: true
#| message: false


queries |>
  dplyr::select(-starts_with("cred")) |> 
  knitr::kable(
    digits = 2,
    booktabs = TRUE,
    align = "c",
    escape = TRUE,
    longtable = TRUE,
    linesep = "") |> 
  kableExtra::kable_classic_2(latex_options = c("hold_position"))
```

\FloatBarrier

{{< pagebreak >}}

## Computational details and software requirements {.unnumbered}

+------------------------:+:-------------------------------------------------------------------------------------+
| Version                 | -   1.1.1                                                                            |
+-------------------------+--------------------------------------------------------------------------------------+
| Availability            | -   Stable Release: <https://cran.rstudio.com/web/packages/CausalQueries/index.html> |
|                         | -   Development: <https://github.com/integrated-inferences/CausalQueries>            |
+-------------------------+--------------------------------------------------------------------------------------+
| Issues                  | -   <https://github.com/integrated-inferences/CausalQueries/issues>                  |
+-------------------------+--------------------------------------------------------------------------------------+
| Operating Systems       | -   Linux                                                                            |
|                         | -   MacOS                                                                            |
|                         | -   Windows                                                                          |
+-------------------------+--------------------------------------------------------------------------------------+
| Testing Environments OS | -   Ubuntu 22.04.2                                                                   |
|                         | -   Debian 12.2                                                                      |
|                         | -   MacOS                                                                            |
|                         | -   Windows                                                                          |
+-------------------------+--------------------------------------------------------------------------------------+
| Testing Environments R  | -   R 4.3.1                                                                          |
|                         | -   R 4.3.0                                                                          |
|                         | -   R 4.2.3                                                                          |
|                         | -   r-devel                                                                          |
+-------------------------+--------------------------------------------------------------------------------------+
| R Version               | -   R(\>= 3.4.0)                                                                     |
+-------------------------+--------------------------------------------------------------------------------------+
| Compiler                | -   either of the below or similar:                                                  |
|                         | -   g++                                                                              |
|                         | -   clang++                                                                          |
+-------------------------+--------------------------------------------------------------------------------------+
| Stan requirements       | -   inline                                                                           |
|                         | -   Rcpp (\>= 0.12.0)                                                                |
|                         | -   RcppEigen (\>= 0.3.3.3.0)                                                        |
|                         | -   RcppArmadillo (\>= 0.12.6.4.0)                                                   |
|                         | -   RcppParallel (\>= 5.1.4)                                                         |
|                         | -   BH (\>= 1.66.0)                                                                  |
|                         | -   StanHeaders (\>= 2.26.0)                                                         |
|                         | -   rstan (\>= 2.26.0)                                                               |
+-------------------------+--------------------------------------------------------------------------------------+
| R-Packages Depends      | -   dplyr                                                                            |
|                         | -   methods                                                                          |
+-------------------------+--------------------------------------------------------------------------------------+
| R-Packages Imports      | -   dagitty (\>= 0.3-1)                                                              |
|                         | -   dirmult (\>= 0.1.3-4)                                                            |
|                         | -   stats (\>= 4.1.1)                                                                |
|                         | -   rlang (\>= 0.2.0)                                                                |
|                         | -   rstan (\>= 2.26.0)                                                               |
|                         | -   rstantools (\>= 2.0.0)                                                           |
|                         | -   stringr (\>= 1.4.0)                                                              |
|                         | -   ggdag (\>= 0.2.4)                                                                |
|                         | -   latex2exp (\>= 0.9.4)                                                            |
|                         | -   ggplot2 (\>= 3.3.5)                                                              |
|                         | -   lifecycle (\>= 1.0.1)                                                            |
+-------------------------+--------------------------------------------------------------------------------------+

: {tbl-colwidths=\[30,70\]}

The results in this paper were obtained using [R]{.proglang}\~3.4.1 with the [MASS]{.pkg}\~7.3.47 package. [R]{.proglang} itself and all packages used are available from the Comprehensive [R]{.proglang} Archive Network (CRAN) at <https://CRAN.R-project.org/>.

## Acknowledgments {.unnumbered}

::: callout
We thank Ben Goodrich, who provided generous insights on using `stan` for this project. We thank Alan M Jacobs for key work in developing the framework underlying the package. Our thanks to Cristian-Liviu Nicolescu, who provided wonderful feedback on the use of the package and a draft of this paper. Our thanks to Jasper Cooper for contributions to the generic function to create Stan code, to [Clara Bicalho](https://clarabicalho.github.io/), who helped figure out the syntax for causal statements, to [Julio S. Sols Arce](https://www.gov.harvard.edu/directory/julio-s-solis-arce/) who made many vital contributions figuring out how to simplify the specification of priors, and to [Merlin Heidemanns](https://merlinheidemanns.github.io/website/) who figured out the `rstantools` integration and made myriad code improvements.
:::

## References {.unnumbered}

::: {#refs}
:::

{{< pagebreak >}}

## Appendix A: Parallelization {#sec-parallel .unnumbered}

If you have multiple cores, you can do parallel processing by including this line before running `CausalQueries`:

```{r}
#| eval: false
#| purl: false
library(parallel)

options(mc.cores = parallel::detectCores())
```

Additionally, parallelizing across models or data while running MCMC chains in parallel can be achieved by setting up a nested parallel process. With 8 cores one can run two updating processes with three parallel chains each simultaneously. More generally the number of parallel processes at the upper level of the nested parallel structure are given by $\left \lfloor \frac{cores}{chains + 1} \right \rfloor$.

```{r}
#| echo: true
#| eval: false
#| purl: false
library(future)
library(future.apply)

chains <- 3
cores <- 8

future::plan(list(
      future::tweak(future::multisession, 
                    workers = floor(cores/(chains + 1))),
      future::tweak(future::multisession, 
                    workers = chains)
    ))

model <- make_model("X -> Y")
data <- list(data_1, data_2)

future.apply::future_lapply(data, function(d) {
  update_model(
    model = model,
    data = d,
    chains = chains,
    refresh = 0
  )
})
```

## Appendix B: Stan code {#sec-stancode .unnumbered}

Updating is performed using a generic Stan model. The data provided to Stan is generated by the internal function `prep_stan_data()`, which returns a list of objects that Stan expects to receive. The code for the Stan model is shown below. After defining a helper function, the code starts with a block declaring what input data is to be expected. Then, the parameters and the transformed parameters are characterized. Then, the likelihoods and priors are provided. At the end, a block for generated quantities is used to append a posterior distribution of causal types to the model.

```{r}
#| echo: false
#| results: markup
#| comment: ""

CausalQueries:::stanmodels$simplexes

```

## Appendix C: Benchmarks {#sec-benchmark .unnumbered}

We present a brief summary of model updating benchmarks. Note that these benchmarks are not generally reproducible and are specific to the specifications of the hardware system used to produce them. The first benchmark considers the effect of model complexity on updating time. The second benchmark considers the effect of data size on updating time. We run $4$ parallel chains for each model. The results of the benchmarks are presented in @tbl-bench1 and @tbl-bench2.

+----------------------------------------------------+----------------------------+--------------------------------------+
| Model                                              | Number of Model Parameters | `update_model()` Runt-Time (seconds) |
+:==================================================:+:==========================:+:====================================:+
| $X1 \rightarrow Y$                                 | 6                          | 10.85                                |
+----------------------------------------------------+----------------------------+--------------------------------------+
| $X1 \rightarrow Y \leftarrow X2$                   | 20                         | 15.79                                |
+----------------------------------------------------+----------------------------+--------------------------------------+
| $X1 \rightarrow Y \leftarrow X2; X3 \rightarrow Y$ | 262                        | 77.56                                |
+----------------------------------------------------+----------------------------+--------------------------------------+

: Benchmark 1. {#tbl-bench1 tbl-colwidths=\[40,30,30\]}

+--------------------+------------------------+----------------------------------------+
| Model              | Number of Observations | `update_model()` Run-Time (seconds) \| |
+:==================:+:======================:+:======================================:+
| $X1 \rightarrow Y$ | 10                     | 9.04                                   |
+--------------------+------------------------+----------------------------------------+
| $X1 \rightarrow Y$ | 100                    | 9.31                                   |
+--------------------+------------------------+----------------------------------------+
| $X1 \rightarrow Y$ | 1000                   | 10.56                                  |
+--------------------+------------------------+----------------------------------------+
| $X1 \rightarrow Y$ | 10000                  | 14.57                                  |
+--------------------+------------------------+----------------------------------------+
| $X1 \rightarrow Y$ | 100000                 | 17.25                                  |
+--------------------+------------------------+----------------------------------------+

```{r, include = FALSE, eval = FALSE}

# effect of model complexity on run-time
model <- list(
  CausalQueries::make_model("X -> Y"),
  CausalQueries::make_model("X1 -> Y <- X2"),
  CausalQueries::make_model("X1 -> Y; X2 -> Y; X3 -> Y")
)

data <- lapply(model, function(m) {
  CausalQueries::make_data(m, 1000)
})

options(mc.cores = parallel::detectCores())

benchmark_model <- microbenchmark::microbenchmark(
  m1 = CausalQueries::update_model(model[[1]], data[[1]]),
  m2 = CausalQueries::update_model(model[[2]], data[[2]]),
  m3 = CausalQueries::update_model(model[[3]], data[[3]]),
  times = 5
)

# effect of data size on run-time
model <- CausalQueries::make_model("X -> Y")

data <- lapply(10^c(1:5), function(n) {
  CausalQueries::make_data(model, n)
})

benchmark_data <- microbenchmark::microbenchmark(
  d0 = CausalQueries::update_model(model, data[[1]]),
  d1 = CausalQueries::update_model(model, data[[2]]),
  d2 = CausalQueries::update_model(model, data[[3]]),
  d3 = CausalQueries::update_model(model, data[[4]]),
  times = 5
)

```

: Benchmark 2. {#tbl-bench2 tbl-colwidths=\[40,30,30\]}

Increasing the number of parents in a model greatly increases the number of parameters and computational time. The growth of the parameter space with increasing model complexity places limits on feasible computability without further recourse to specialized methods for handling large causal models. Data size increases here have a more modest effect on computation time.
